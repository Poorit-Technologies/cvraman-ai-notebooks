{
 "cells": [
  {
   "cell_type": "markdown",
   "source": "> **Pre-session prep:** Before opening this notebook, make sure students understand `pip install`, `getpass`, f-strings, and classes. See **[teaching-guide.md](teaching-guide.md)** for verbal talking points organized by teaching order.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "<img src=\"https://poorit.in/image.png\" alt=\"Poorit\" width=\"40\" style=\"vertical-align: middle;\"> <b>AI SYSTEMS ENGINEERING 1</b>\n",
    "\n",
    "## Revision Session: Unit 1 + Unit 2 (Gradio)\n",
    "\n",
    "**CV Raman Global University, Bhubaneswar**  \n",
    "*AI Center of Excellence*\n",
    "\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "### Session Structure: 5 Acts in 120 Minutes\n",
    "\n",
    "| Act | Topic | Analogy | Time |\n",
    "|-----|-------|---------|------|\n",
    "| 1 | API Calls | The Restaurant Order | 30 min |\n",
    "| 2 | Tokens & Memory | The Memory Illusion | 25 min |\n",
    "| 3 | JSON & Chaining | The Assembly Line | 20 min |\n",
    "| 4 | Streaming | Live Cricket Commentary | 20 min |\n",
    "| 5 | Gradio UIs | The Shop Counter | 25 min |\n",
    "\n",
    "**Format:** I Do (watch) → We Do (predict) → You Do (code)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Run the cells below to install packages and configure API keys.\n",
    "This setup works in both Google Colab and local Jupyter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q openai tiktoken requests beautifulsoup4 gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from getpass import getpass\n",
    "from openai import OpenAI\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import tiktoken\n",
    "import gradio as gr\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure API Keys\n",
    "openai_api_key = getpass(\"Enter your OpenAI API Key: \")\n",
    "os.environ['OPENAI_API_KEY'] = openai_api_key\n",
    "\n",
    "# OpenAI client\n",
    "openai_client = OpenAI(api_key=openai_api_key)\n",
    "MODEL = \"gpt-4o-mini\"\n",
    "print(f\"OpenAI configured with model: {MODEL}\")\n",
    "\n",
    "# Optional: Gemini client\n",
    "google_api_key = getpass(\"Enter your Google API Key (or press Enter to skip): \")\n",
    "GEMINI_BASE_URL = \"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    "gemini_client = OpenAI(base_url=GEMINI_BASE_URL, api_key=google_api_key) if google_api_key else None\n",
    "if gemini_client:\n",
    "    print(\"Gemini configured\")\n",
    "else:\n",
    "    print(\"Gemini skipped (no key provided)\")\n",
    "\n",
    "# We'll use openai_client as the default\n",
    "client = openai_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Web scraping utility — reused across multiple acts\n",
    "\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36\"\n",
    "}\n",
    "\n",
    "def fetch_website_contents(url, max_chars=2000):\n",
    "    \"\"\"\n",
    "    Fetch and return the title and text content of a website.\n",
    "    Removes scripts, styles, and other non-text elements.\n",
    "    \"\"\"\n",
    "    response = requests.get(url, headers=HEADERS, timeout=10)\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    \n",
    "    title = soup.title.string if soup.title else \"No title found\"\n",
    "    \n",
    "    if soup.body:\n",
    "        for irrelevant in soup.body([\"script\", \"style\", \"img\", \"input\"]):\n",
    "            irrelevant.decompose()\n",
    "        text = soup.body.get_text(separator=\"\\n\", strip=True)\n",
    "    else:\n",
    "        text = \"\"\n",
    "    \n",
    "    return (title + \"\\n\\n\" + text)[:max_chars]\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Act 1: The Restaurant Order — How LLM API Calls Work\n",
    "\n",
    "**Covers:** Notebooks 1 + 2 | **Time:** 30 minutes\n",
    "\n",
    "---\n",
    "\n",
    "### The Analogy\n",
    "\n",
    "Think of an LLM API call like ordering at a restaurant:\n",
    "\n",
    "| Restaurant | Code |\n",
    "|---|---|\n",
    "| Which restaurant | `model=\"gpt-4o-mini\"` |\n",
    "| The waiter (takes your order) | `client = OpenAI()` |\n",
    "| Your order slip | `messages=[{role, content}, ...]` |\n",
    "| Chef's standing instructions | System prompt |\n",
    "| Your actual request | User prompt |\n",
    "| Food delivered | `response.choices[0].message.content` |\n",
    "| The bill | Tokens (input + output) |\n",
    "| Same waiter, different kitchen | `base_url` change for Gemini/Ollama |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I Do: Your First API Call (Professor live codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Create the messages list\n",
    "# The API expects a list of messages, each with a \"role\" and \"content\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What is the capital of Odisha?\"}\n",
    "]\n",
    "\n",
    "# Step 2: Make the API call\n",
    "response = client.chat.completions.create(\n",
    "    model=MODEL,\n",
    "    messages=messages\n",
    ")\n",
    "\n",
    "# Step 3: Extract the response\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We Do: What does each part do?\n",
    "\n",
    "Before running the next cell, predict:\n",
    "- What role should the system prompt have?\n",
    "- What goes in the user message?\n",
    "- Where is the response text stored?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see what the raw response object looks like\n",
    "print(\"Full response object:\")\n",
    "print(response)\n",
    "print()\n",
    "print(\"--- Breaking it down ---\")\n",
    "print(f\"Model used: {response.model}\")\n",
    "print(f\"Choices count: {len(response.choices)}\")\n",
    "print(f\"Message role: {response.choices[0].message.role}\")\n",
    "print(f\"Message content: {response.choices[0].message.content}\")\n",
    "print(f\"Tokens used - Input: {response.usage.prompt_tokens}, Output: {response.usage.completion_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why does content go in the user prompt (not system)?\n",
    "\n",
    "The **system prompt** = chef's standing instructions (\"always serve vegetarian\")  \n",
    "The **user prompt** = each specific order (\"I want biryani\")\n",
    "\n",
    "When summarizing a website, the website text goes in the **user prompt** because it's the specific \"order\" — the content to process. The system prompt just says HOW to process it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You Do: System Prompt Swap\n",
    "\n",
    "**Task:** Change the system prompt to make the model respond differently. Try:\n",
    "1. First: Make it respond like a pirate\n",
    "2. Then: Make it respond only in Hindi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR TURN: Change the system prompt\n",
    "# Try: \"You are a pirate. Respond to everything in pirate speak.\"\n",
    "# Or:  \"You are a helpful assistant. Always respond in Hindi.\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"___\"},  # <-- Change this!\n",
    "    {\"role\": \"user\", \"content\": \"What is machine learning?\"}\n",
    "]\n",
    "\n",
    "response = client.chat.completions.create(model=MODEL, messages=messages)\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I Do: Same Waiter, Different Kitchen (Provider Switch)\n",
    "\n",
    "The OpenAI client library can talk to ANY provider that supports the same API format. We just change the `base_url` — like sending the same waiter to a different kitchen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI (default kitchen)\n",
    "print(\"--- OpenAI ---\")\n",
    "response = openai_client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Tell me a fun fact about Bhubaneswar in one sentence.\"}]\n",
    ")\n",
    "print(response.choices[0].message.content)\n",
    "\n",
    "# Gemini (different kitchen, same waiter!)\n",
    "if gemini_client:\n",
    "    print(\"\\n--- Gemini ---\")\n",
    "    response = gemini_client.chat.completions.create(\n",
    "        model=\"gemini-2.0-flash\",\n",
    "        messages=[{\"role\": \"user\", \"content\": \"Tell me a fun fact about Bhubaneswar in one sentence.\"}]\n",
    "    )\n",
    "    print(response.choices[0].message.content)\n",
    "else:\n",
    "    print(\"\\nGemini not configured — skipped\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I Do: Client Library vs Raw HTTP (Quick Demo)\n",
    "\n",
    "The client library is just a convenient wrapper around raw HTTP calls. Here's what it does under the hood:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What the OpenAI library does for you behind the scenes:\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {openai_api_key}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "payload = {\n",
    "    \"model\": \"gpt-4o-mini\",\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"Say hello in one word\"}]\n",
    "}\n",
    "\n",
    "raw_response = requests.post(\n",
    "    \"https://api.openai.com/v1/chat/completions\",\n",
    "    headers=headers,\n",
    "    json=payload\n",
    ")\n",
    "\n",
    "# With raw HTTP, you get a dictionary — notice the different syntax:\n",
    "print(\"Raw HTTP:\", raw_response.json()[\"choices\"][0][\"message\"][\"content\"])\n",
    "# vs client library: response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Act 2: The Memory Illusion — Tokens & Statelessness\n",
    "\n",
    "**Covers:** Notebook 3 | **Time:** 25 minutes\n",
    "\n",
    "---\n",
    "\n",
    "### Key Insight\n",
    "\n",
    "LLMs have **zero memory**. Every API call is completely independent. When ChatGPT seems to \"remember\" your conversation, it's actually sending the **entire chat history** with every single message. That's why long conversations cost more!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I Do: How LLMs See Your Text (Tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tiktoken: OpenAI's tokenizer library\n",
    "encoding = tiktoken.encoding_for_model(\"gpt-4o-mini\")\n",
    "\n",
    "# English vs Hindi — different token counts!\n",
    "for text in [\"Hello\", \"Namaste\", \"नमस्ते\", \"Bhubaneswar\", \"Machine Learning\"]:\n",
    "    tokens = encoding.encode(text)\n",
    "    print(f\"'{text}' → {len(tokens)} tokens  (IDs: {tokens})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See what each token represents\n",
    "text = \"Hello, my name is Ravi\"\n",
    "tokens = encoding.encode(text)\n",
    "print(f\"Text: '{text}'\")\n",
    "print(f\"Total tokens: {len(tokens)}\")\n",
    "print()\n",
    "for token_id in tokens:\n",
    "    print(f\"  {token_id:6d} → '{encoding.decode([token_id])}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I Do: The Memory Illusion (THE key demo)\n",
    "\n",
    "Watch what happens when we make two separate API calls:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call 1: Introduce ourselves\n",
    "print(\"--- Call 1: Introducing ourselves ---\")\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "    {\"role\": \"user\", \"content\": \"Hi! My name is Priya!\"}\n",
    "]\n",
    "response = client.chat.completions.create(model=MODEL, messages=messages)\n",
    "call_1_reply = response.choices[0].message.content\n",
    "print(f\"Assistant: {call_1_reply}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call 2: Ask for our name — WITHOUT history\n",
    "print(\"--- Call 2: Asking our name (NEW call, no history) ---\")\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "    {\"role\": \"user\", \"content\": \"What's my name?\"}\n",
    "]\n",
    "response = client.chat.completions.create(model=MODEL, messages=messages)\n",
    "print(f\"Assistant: {response.choices[0].message.content}\")\n",
    "print()\n",
    "print(\"^^^ It DOESN'T remember! Each call is stateless.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We Do: Before running the next cell...\n",
    "\n",
    "**Predict:** What will happen if we include the full conversation history in Call 3?  \n",
    "**Think:** Why does this work? What does the `assistant` role do in the messages list?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call 3: Ask for our name — WITH full history\n",
    "print(\"--- Call 3: Asking our name (WITH history) ---\")\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "    {\"role\": \"user\", \"content\": \"Hi! My name is Priya!\"},\n",
    "    {\"role\": \"assistant\", \"content\": call_1_reply},  # Include what the model said before\n",
    "    {\"role\": \"user\", \"content\": \"What's my name?\"}\n",
    "]\n",
    "response = client.chat.completions.create(model=MODEL, messages=messages)\n",
    "print(f\"Assistant: {response.choices[0].message.content}\")\n",
    "print()\n",
    "print(\"^^^ Now it 'remembers'! We sent the full conversation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Context Window & Cost\n",
    "\n",
    "Everything must fit in the **context window**:\n",
    "\n",
    "```\n",
    "System Prompt + Conversation History + Current Message + Response\n",
    "= Must all fit within the context window (e.g., 128K tokens for GPT-4o-mini)\n",
    "```\n",
    "\n",
    "**Cost formula:**\n",
    "```\n",
    "Cost = (input_tokens / 1,000,000) × input_price + (output_tokens / 1,000,000) × output_price\n",
    "```\n",
    "\n",
    "GPT-4o-mini: $0.15 per 1M input tokens, $0.60 per 1M output tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You Do: Build the Memory Yourself\n",
    "\n",
    "**Task:** Complete the 3 cells below to experience statelessness firsthand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Introduce yourself to the model\n",
    "# Replace ___ with your name\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "    {\"role\": \"user\", \"content\": \"Hi! My name is ___\"}  # <-- Your name here\n",
    "]\n",
    "\n",
    "response = client.chat.completions.create(model=MODEL, messages=messages)\n",
    "intro_reply = response.choices[0].message.content\n",
    "print(f\"Assistant: {intro_reply}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Ask your name WITHOUT history — observe the failure\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "    {\"role\": \"user\", \"content\": \"What is my name?\"}\n",
    "]\n",
    "\n",
    "response = client.chat.completions.create(model=MODEL, messages=messages)\n",
    "print(f\"Assistant: {response.choices[0].message.content}\")\n",
    "# It won't know your name!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Now build the messages list WITH history — observe success\n",
    "# Fill in the ___ parts\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "    {\"role\": \"___\", \"content\": \"Hi! My name is ___\"},        # <-- What role? What name?\n",
    "    {\"role\": \"___\", \"content\": intro_reply},                   # <-- What role for the model's reply?\n",
    "    {\"role\": \"___\", \"content\": \"What is my name?\"}             # <-- What role?\n",
    "]\n",
    "\n",
    "response = client.chat.completions.create(model=MODEL, messages=messages)\n",
    "print(f\"Assistant: {response.choices[0].message.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Act 3: The Assembly Line — JSON & Chaining\n",
    "\n",
    "**Covers:** Notebook 4 concepts | **Time:** 20 minutes\n",
    "\n",
    "---\n",
    "\n",
    "### The Analogy\n",
    "\n",
    "Think of a factory assembly line:\n",
    "\n",
    "| Factory | Code |\n",
    "|---|---|\n",
    "| Station 1: Classifier worker | LLM Call 1 → JSON output |\n",
    "| Labeled box between stations | `response_format={\"type\": \"json_object\"}` |\n",
    "| Station 2: Fetcher worker | Your code calls `fetch_website_contents` |\n",
    "| Station 3: Writer worker | LLM Call 2 generates pamphlet |\n",
    "\n",
    "**Why JSON?** Because Station 2 is CODE, not a human. Code needs structured data it can parse reliably.\n",
    "\n",
    "This is an early form of **Agentic AI** — LLM output decides what the code does next!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I Do: JSON Mode (Professor live codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask the model to return structured data as JSON\n",
    "response = client.chat.completions.create(\n",
    "    model=MODEL,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": 'Respond in JSON with keys: topic, summary, difficulty. Example: {\"topic\": \"AI\", \"summary\": \"...\", \"difficulty\": \"beginner\"}'},\n",
    "        {\"role\": \"user\", \"content\": \"Explain what web scraping is\"}\n",
    "    ],\n",
    "    response_format={\"type\": \"json_object\"}   # <-- This forces JSON output\n",
    ")\n",
    "\n",
    "# The response is a JSON STRING — we need to parse it\n",
    "raw_text = response.choices[0].message.content\n",
    "print(\"Raw response (it's a string):\")\n",
    "print(raw_text)\n",
    "print(f\"Type: {type(raw_text)}\")\n",
    "\n",
    "print()\n",
    "\n",
    "# json.loads() converts string → Python dictionary\n",
    "result = json.loads(raw_text)\n",
    "print(\"Parsed result (it's a dict now):\")\n",
    "print(f\"Topic: {result['topic']}\")\n",
    "print(f\"Difficulty: {result['difficulty']}\")\n",
    "print(f\"Type: {type(result)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We Do: What keys should we request?\n",
    "\n",
    "If we wanted the model to analyze a college course, what JSON keys would be useful?\n",
    "\n",
    "Think of 3-4 keys (e.g., `course_name`, `prerequisites`, ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try it with the keys the class suggested!\n",
    "response = client.chat.completions.create(\n",
    "    model=MODEL,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": 'Analyze the course and respond in JSON with keys: course_name, prerequisites, difficulty, career_paths'},\n",
    "        {\"role\": \"user\", \"content\": \"B.Tech in Computer Science with specialization in AI\"}\n",
    "    ],\n",
    "    response_format={\"type\": \"json_object\"}\n",
    ")\n",
    "\n",
    "result = json.loads(response.choices[0].message.content)\n",
    "print(json.dumps(result, indent=2))  # Pretty print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I Do: Walking Through the Pipeline (Notebook 4 recap)\n",
    "\n",
    "In Notebook 4, we built a pamphlet generator with this pipeline:\n",
    "\n",
    "```\n",
    "LLM Call 1: \"Pick relevant links from this website\" (returns JSON)\n",
    "     ↓\n",
    "Code: fetch_website_contents() for each selected link\n",
    "     ↓  \n",
    "LLM Call 2: \"Write a pamphlet from this content\" (returns markdown)\n",
    "```\n",
    "\n",
    "The LLM's output (selected links) **decides** what the code fetches next. That's the \"agentic\" part — the LLM is making decisions that drive the program."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Act 4: Live Cricket Commentary — Streaming & Generators\n",
    "\n",
    "**Covers:** Notebook 4 streaming + Python generators | **Time:** 20 minutes\n",
    "\n",
    "---\n",
    "\n",
    "### The Analogy\n",
    "\n",
    "| Cricket | Code |\n",
    "|---|---|\n",
    "| Full match highlights (wait, then watch all) | Normal API call (synchronous) |\n",
    "| Ball-by-ball commentary (live!) | `stream=True` |\n",
    "| Each ball delivery | `chunk.choices[0].delta.content` |\n",
    "| Scoreboard updating live | `yield` to UI |\n",
    "| `return` = full highlights DVD | Returns one complete value |\n",
    "| `yield` = live commentary mic | Returns values one at a time (generator) |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I Do: return vs yield (Professor demo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RETURN: Gives you the full result at the end (match highlights)\n",
    "def get_commentary_return():\n",
    "    commentary = \"\"\n",
    "    for ball in [\"Ball 1: Dot ball. \", \"Ball 2: Four! \", \"Ball 3: Six! \", \"Ball 4: Wicket! \"]:\n",
    "        commentary += ball\n",
    "    return commentary  # You get EVERYTHING at once, at the end\n",
    "\n",
    "result = get_commentary_return()\n",
    "print(\"With return (all at once):\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# YIELD: Gives you each piece as it happens (live commentary)\n",
    "def get_commentary_yield():\n",
    "    commentary = \"\"\n",
    "    for ball in [\"Ball 1: Dot ball. \", \"Ball 2: Four! \", \"Ball 3: Six! \", \"Ball 4: Wicket! \"]:\n",
    "        commentary += ball\n",
    "        yield commentary  # Send the current state RIGHT NOW\n",
    "\n",
    "print(\"With yield (one at a time):\")\n",
    "for update in get_commentary_yield():\n",
    "    print(update)\n",
    "    time.sleep(0.5)  # Simulate delay between balls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I Do: Streaming from the API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non-streaming: wait... wait... then get everything\n",
    "print(\"--- Non-streaming (match highlights) ---\")\n",
    "response = client.chat.completions.create(\n",
    "    model=MODEL,\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Write a 3-line poem about cricket.\"}]\n",
    ")\n",
    "print(response.choices[0].message.content)  # .message.content for normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Streaming: text flows in word by word!\n",
    "print(\"--- Streaming (live commentary) ---\")\n",
    "stream = client.chat.completions.create(\n",
    "    model=MODEL,\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Write a 3-line poem about cricket.\"}],\n",
    "    stream=True  # <-- This is the only change!\n",
    ")\n",
    "\n",
    "for chunk in stream:\n",
    "    content = chunk.choices[0].delta.content or \"\"  # .delta.content for streaming\n",
    "    print(content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You Do: Add Streaming to a Working Call\n",
    "\n",
    "**Task:** The cell below has a working non-streaming API call. Convert it to streaming by:\n",
    "1. Adding `stream=True`\n",
    "2. Writing the chunk loop\n",
    "3. Printing each chunk as it arrives\n",
    "\n",
    "Fill in the `___` parts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR TURN: Convert this to streaming\n",
    "\n",
    "# This is the non-streaming version (works but waits):\n",
    "# response = client.chat.completions.create(\n",
    "#     model=MODEL,\n",
    "#     messages=[{\"role\": \"user\", \"content\": \"Explain AI in 3 sentences.\"}]\n",
    "# )\n",
    "# print(response.choices[0].message.content)\n",
    "\n",
    "# Convert to streaming — fill in the blanks:\n",
    "stream = client.chat.completions.create(\n",
    "    model=MODEL,\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Explain AI in 3 sentences.\"}],\n",
    "    ___=True  # <-- What parameter enables streaming?\n",
    ")\n",
    "\n",
    "for ___ in stream:  # <-- What variable holds each piece?\n",
    "    content = ___.choices[0].___.content or \"\"  # <-- .message or .delta?\n",
    "    print(content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Act 5: The Shop Counter — Gradio UIs\n",
    "\n",
    "**Covers:** Unit 2 Gradio notebooks | **Time:** 25 minutes\n",
    "\n",
    "---\n",
    "\n",
    "### The Analogy\n",
    "\n",
    "| Shop | Code |\n",
    "|---|---|\n",
    "| Your cooking function | Python function |\n",
    "| Counter setup | `gr.Interface(fn, inputs, outputs)` |\n",
    "| Menu board with examples | `examples=[...]` |\n",
    "| Fancy restaurant with order history | `gr.ChatInterface(fn, type=\"messages\")` |\n",
    "| Waiter's notepad | `history` parameter |\n",
    "| Adjusting recipe based on customer | Dynamic system prompts |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I Do: Step 1 — Simplest Gradio App"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Any Python function can become a web app\n",
    "def shout(text):\n",
    "    return text.upper()\n",
    "\n",
    "gr.Interface(fn=shout, inputs=\"textbox\", outputs=\"textbox\", flagging_mode=\"never\").launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I Do: Step 2 — Connect to an LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Replace our simple function with an LLM call\n",
    "def message_gpt(prompt):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant. Respond in markdown.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "    response = client.chat.completions.create(model=MODEL, messages=messages)\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "gr.Interface(\n",
    "    fn=message_gpt,\n",
    "    inputs=gr.Textbox(label=\"Your message:\", lines=3),\n",
    "    outputs=gr.Markdown(label=\"Response:\"),\n",
    "    title=\"GPT Assistant\",\n",
    "    examples=[\"Explain recursion simply\", \"What is the capital of Odisha?\"],\n",
    "    flagging_mode=\"never\"\n",
    ").launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I Do: Step 3 — Add Streaming with yield"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Add streaming — just change return to yield!\n",
    "def stream_gpt(prompt):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant. Respond in markdown.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "    stream = client.chat.completions.create(model=MODEL, messages=messages, stream=True)\n",
    "    \n",
    "    result = \"\"\n",
    "    for chunk in stream:\n",
    "        result += chunk.choices[0].delta.content or \"\"\n",
    "        yield result  # Gradio picks up each yield and updates the UI!\n",
    "\n",
    "gr.Interface(\n",
    "    fn=stream_gpt,\n",
    "    inputs=gr.Textbox(label=\"Your message:\", lines=3),\n",
    "    outputs=gr.Markdown(label=\"Response:\"),\n",
    "    title=\"GPT Assistant (Streaming)\",\n",
    "    examples=[\"Write a poem about coding\", \"Explain neural networks\"],\n",
    "    flagging_mode=\"never\"\n",
    ").launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I Do: Step 4 — Add a Dropdown for Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Add a dropdown to choose tone\n",
    "def stream_with_tone(prompt, tone):\n",
    "    system_prompts = {\n",
    "        \"Formal\": \"You are a formal, professional assistant. Respond in markdown.\",\n",
    "        \"Funny\": \"You are a witty, humorous assistant. Use jokes and puns. Respond in markdown.\",\n",
    "        \"Simple\": \"You are an assistant that explains things as simply as possible, like to a 10-year-old. Respond in markdown.\"\n",
    "    }\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompts[tone]},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "    stream = client.chat.completions.create(model=MODEL, messages=messages, stream=True)\n",
    "    \n",
    "    result = \"\"\n",
    "    for chunk in stream:\n",
    "        result += chunk.choices[0].delta.content or \"\"\n",
    "        yield result\n",
    "\n",
    "gr.Interface(\n",
    "    fn=stream_with_tone,\n",
    "    inputs=[\n",
    "        gr.Textbox(label=\"Your message:\", lines=3),\n",
    "        gr.Dropdown([\"Formal\", \"Funny\", \"Simple\"], label=\"Tone\", value=\"Formal\")\n",
    "    ],\n",
    "    outputs=gr.Markdown(label=\"Response:\"),\n",
    "    title=\"Multi-Tone Assistant\",\n",
    "    examples=[\n",
    "        [\"Explain machine learning\", \"Formal\"],\n",
    "        [\"Explain machine learning\", \"Funny\"],\n",
    "        [\"Explain machine learning\", \"Simple\"]\n",
    "    ],\n",
    "    flagging_mode=\"never\"\n",
    ").launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I Do: ChatInterface (Conversations with Memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gr.ChatInterface handles history automatically!\n",
    "def chat(message, history):\n",
    "    # Convert Gradio's history format to OpenAI's format\n",
    "    history = [{\"role\": h[\"role\"], \"content\": h[\"content\"]} for h in history]\n",
    "    \n",
    "    # Build the full messages list: system + history + current message\n",
    "    messages = (\n",
    "        [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}]\n",
    "        + history\n",
    "        + [{\"role\": \"user\", \"content\": message}]\n",
    "    )\n",
    "    \n",
    "    # Stream the response\n",
    "    stream = client.chat.completions.create(model=MODEL, messages=messages, stream=True)\n",
    "    \n",
    "    response = \"\"\n",
    "    for chunk in stream:\n",
    "        response += chunk.choices[0].delta.content or \"\"\n",
    "        yield response\n",
    "\n",
    "gr.ChatInterface(fn=chat, type=\"messages\").launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You Do: Build a Gradio App (Culminating Exercise)\n",
    "\n",
    "**Task:** Build a Gradio app that:\n",
    "1. Takes a **topic** (textbox) and a **tone** (dropdown: Formal / Funny / Simple)\n",
    "2. Generates an explanation of the topic in the selected tone\n",
    "3. Uses **streaming** (yield, not return)\n",
    "\n",
    "Fill in the `___` parts below. This tests everything from Acts 1-5!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR TURN: Build a complete Gradio app\n",
    "\n",
    "def explain_topic(topic, tone):\n",
    "    # Step 1: Choose system prompt based on tone\n",
    "    if tone == \"Formal\":\n",
    "        system = \"You are a formal professor. Explain topics clearly and professionally. Respond in markdown.\"\n",
    "    elif tone == \"Funny\":\n",
    "        system = \"You are a comedian who explains topics with humor and jokes. Respond in markdown.\"\n",
    "    else:\n",
    "        system = \"You explain topics as simply as possible, like to a 10-year-old. Respond in markdown.\"\n",
    "    \n",
    "    # Step 2: Create the messages list\n",
    "    messages = [\n",
    "        {\"role\": \"___\", \"content\": system},             # <-- What role?\n",
    "        {\"role\": \"___\", \"content\": f\"Explain: {topic}\"}  # <-- What role?\n",
    "    ]\n",
    "    \n",
    "    # Step 3: Make the streaming API call\n",
    "    stream = client.chat.completions.create(\n",
    "        model=MODEL,\n",
    "        messages=messages,\n",
    "        ___=True  # <-- What parameter enables streaming?\n",
    "    )\n",
    "    \n",
    "    # Step 4: Yield results as they arrive\n",
    "    result = \"\"\n",
    "    for chunk in stream:\n",
    "        result += chunk.choices[0].___.content or \"\"  # <-- .message or .delta?\n",
    "        ___ result  # <-- return or yield?\n",
    "\n",
    "# Step 5: Create the Gradio interface\n",
    "gr.Interface(\n",
    "    fn=___,  # <-- Which function?\n",
    "    inputs=[\n",
    "        gr.Textbox(label=\"Topic\", lines=2),\n",
    "        gr.Dropdown([\"Formal\", \"Funny\", \"Simple\"], label=\"Tone\", value=\"Formal\")\n",
    "    ],\n",
    "    outputs=gr.Markdown(label=\"Explanation\"),\n",
    "    title=\"Topic Explainer\",\n",
    "    flagging_mode=\"never\"\n",
    ").launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## The Big Picture\n",
    "\n",
    "Here's the full journey you've completed:\n",
    "\n",
    "```\n",
    "Notebook 1: API calls + Web scraping\n",
    "     ↓\n",
    "Notebook 2: Multiple providers (same interface!)\n",
    "     ↓\n",
    "Notebook 3: Tokens, costs, memory illusion\n",
    "     ↓\n",
    "Notebook 4: JSON + Chaining + Streaming\n",
    "     ↓\n",
    "Unit 2: Gradio UIs (Interface + ChatInterface)\n",
    "```\n",
    "\n",
    "### Cross-cutting patterns that appear EVERYWHERE:\n",
    "\n",
    "1. **The API call pattern** — same `client.chat.completions.create()` in every notebook\n",
    "2. **`fetch_website_contents`** — reused across notebooks 1, 4, and Gradio\n",
    "3. **Progression**: synchronous → streaming → streaming in Gradio UI\n",
    "4. **The wrapper function pattern** — every notebook wraps lower-level functions into higher-level ones\n",
    "\n",
    "### You can now:\n",
    "- Call any LLM (OpenAI, Gemini, local via Ollama)\n",
    "- Scrape and process web content\n",
    "- Chain multiple LLM calls into pipelines\n",
    "- Stream responses for better UX\n",
    "- Build interactive UIs with Gradio\n",
    "\n",
    "**That's AI Systems Engineering.**\n",
    "\n",
    "---\n",
    "\n",
    "**Course Information:**\n",
    "- **Institution:** CV Raman Global University, Bhubaneswar\n",
    "- **Program:** AI Center of Excellence\n",
    "- **Course:** AI Systems Engineering 1\n",
    "- **Developed by:** [Poorit Technologies](https://poorit.in) - *Transform Graduates into Industry-Ready Professionals*\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}