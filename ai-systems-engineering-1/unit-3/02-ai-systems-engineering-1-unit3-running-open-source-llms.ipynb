{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "<img src=\"https://poorit.in/image.png\" alt=\"Poorit\" width=\"40\" style=\"vertical-align: middle;\"> <b>AI SYSTEMS ENGINEERING 1</b>\n",
    "\n",
    "## Unit 3: Running Open-Source LLMs\n",
    "\n",
    "**CV Raman Global University, Bhubaneswar**  \n",
    "*AI Center of Excellence*\n",
    "\n",
    "---\n",
    "\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "### What You'll Learn\n",
    "\n",
    "In this notebook, you will:\n",
    "\n",
    "1. **Load and run an open-source LLM** (GPT-2) using the Transformers library\n",
    "2. **Understand generation parameters** \u2014 temperature, top_p, sampling\n",
    "\n",
    "**Duration:** ~30 minutes\n",
    "\n",
    "**Note:** This notebook works on CPU but runs faster on GPU. On Colab, go to **Runtime > Change runtime type > T4 GPU**.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q transformers accelerate torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Loading a Small Model (GPT-2)\n",
    "\n",
    "In notebook 01 we used the `pipeline` API. Here we'll load the model and tokenizer directly, which gives us more control.\n",
    "\n",
    "GPT-2 has 124M parameters \u2014 small enough to run on CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GPT-2 model and tokenizer\n",
    "model_name = \"gpt2\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Move to GPU if available\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate text\n",
    "prompt = \"India is a country known for\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=50,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Understanding Generation Parameters\n",
    "\n",
    "When generating text, several parameters control the output:\n",
    "\n",
    "| Parameter | Description | Effect |\n",
    "|-----------|-------------|--------|\n",
    "| `max_new_tokens` | Maximum tokens to generate | Controls output length |\n",
    "| `temperature` | Randomness (0.0\u20132.0) | Higher = more creative/random |\n",
    "| `top_p` | Nucleus sampling threshold | Limits token choices to top probability mass |\n",
    "| `do_sample` | Enable sampling | `False` = greedy (always picks most likely token) |\n",
    "\n",
    "Let's see how **temperature** affects the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different temperatures\n",
    "prompt = \"The future of artificial intelligence is\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "for temp in [0.3, 0.7, 1.2]:\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=30,\n",
    "        do_sample=True,\n",
    "        temperature=temp,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    print(f\"Temperature {temp}:\")\n",
    "    print(f\"  {text}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What you should notice:**\n",
    "- **Low temperature (0.3)** \u2014 more focused and repetitive\n",
    "- **Medium temperature (0.7)** \u2014 good balance of coherence and variety\n",
    "- **High temperature (1.2)** \u2014 more creative but less predictable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Exercise\n",
    "\n",
    "Experiment with generation parameters! Change the `temperature` and `max_new_tokens` below and run the cell multiple times to see how the output changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise: Change the parameters and run this cell multiple times\n",
    "# Try: temperature=0.2, temperature=1.5, max_new_tokens=50 vs 200\n",
    "\n",
    "prompt = \"Write a short poem about coding:\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=100,   # Try changing this\n",
    "    do_sample=True,\n",
    "    temperature=0.7,      # Try changing this\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **`AutoModelForCausalLM` + `AutoTokenizer`** let you load any text generation model directly, giving you full control over tokenization and generation\n",
    "\n",
    "2. **Temperature controls randomness** \u2014 low values (0.2\u20130.4) produce focused, predictable text; high values (1.0+) produce creative, varied text\n",
    "\n",
    "---\n",
    "\n",
    "## Additional Resources\n",
    "\n",
    "- [HuggingFace Model Hub](https://huggingface.co/models)\n",
    "- [Text Generation Strategies](https://huggingface.co/docs/transformers/generation_strategies)\n",
    "\n",
    "---\n",
    "\n",
    "**Course Information:**\n",
    "- **Institution:** CV Raman Global University, Bhubaneswar\n",
    "- **Program:** AI Center of Excellence\n",
    "- **Course:** AI Systems Engineering 1\n",
    "- **Developed by:** [Poorit Technologies](https://poorit.in) \u2014 *Transform Graduates into Industry-Ready Professionals*\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
