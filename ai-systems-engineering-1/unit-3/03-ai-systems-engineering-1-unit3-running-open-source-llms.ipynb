{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "<img src=\"https://poorit.in/image.png\" alt=\"Poorit\" width=\"40\" style=\"vertical-align: middle;\"> <b>AI SYSTEMS ENGINEERING 1</b>\n",
    "\n",
    "## Unit 3: Running Open-Source LLMs\n",
    "\n",
    "**CV Raman Global University, Bhubaneswar**  \n",
    "*AI Center of Excellence*\n",
    "\n",
    "---\n",
    "\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "### What You'll Learn\n",
    "\n",
    "In this notebook, you will:\n",
    "\n",
    "1. **Load and run an open-source LLM** (GPT-2) using the Transformers library\n",
    "2. **Understand generation parameters** — temperature, top_p, sampling\n",
    "3. **Use a chat-style model** (TinyLlama) for instruction-following\n",
    "\n",
    "**Duration:** ~45 minutes\n",
    "\n",
    "**Note:** This notebook works on CPU but runs faster on GPU. On Colab, go to **Runtime > Change runtime type > T4 GPU**.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q transformers accelerate torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n\n# Check if GPU is available\nif torch.cuda.is_available():\n    print(f\"GPU available: {torch.cuda.get_device_name(0)}\")\n    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\nelse:\n    print(\"Running on CPU — this is fine! Models will just be a bit slower.\")\n    print(\"For faster inference, use Colab with a T4 GPU runtime.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Loading a Small Model (GPT-2)\n",
    "\n",
    "In notebook 01 we used the `pipeline` API. Here we'll load the model and tokenizer directly, which gives us more control.\n",
    "\n",
    "GPT-2 has 124M parameters — small enough to run on CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GPT-2 model and tokenizer\n",
    "model_name = \"gpt2\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Move to GPU if available\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = model.to(device)\n",
    "\n",
    "print(f\"Model loaded on: {device}\")\n",
    "print(f\"Parameters: {model.num_parameters() / 1e6:.1f}M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate text\n",
    "prompt = \"India is a country known for\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=50,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Understanding Generation Parameters\n",
    "\n",
    "When generating text, several parameters control the output:\n",
    "\n",
    "| Parameter | Description | Effect |\n",
    "|-----------|-------------|--------|\n",
    "| `max_new_tokens` | Maximum tokens to generate | Controls output length |\n",
    "| `temperature` | Randomness (0.0–2.0) | Higher = more creative/random |\n",
    "| `top_p` | Nucleus sampling threshold | Limits token choices to top probability mass |\n",
    "| `do_sample` | Enable sampling | `False` = greedy (always picks most likely token) |\n",
    "\n",
    "Let's see how **temperature** affects the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different temperatures\n",
    "prompt = \"The future of artificial intelligence is\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "for temp in [0.3, 0.7, 1.2]:\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=30,\n",
    "        do_sample=True,\n",
    "        temperature=temp,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    print(f\"Temperature {temp}:\")\n",
    "    print(f\"  {text}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What you should notice:**\n",
    "- **Low temperature (0.3)** — more focused and repetitive\n",
    "- **Medium temperature (0.7)** — good balance of coherence and variety\n",
    "- **High temperature (1.2)** — more creative but less predictable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Chat-Style Models (TinyLlama)\n",
    "\n",
    "GPT-2 is a base model — it just predicts the next word. Modern models like TinyLlama are **instruction-tuned** and can follow chat-style prompts.\n",
    "\n",
    "TinyLlama (1.1B parameters) is small enough for CPU but much more capable than GPT-2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load TinyLlama using the pipeline API (simpler than manual loading)\n",
    "chat = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
    "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "    device=0 if torch.cuda.is_available() else -1\n",
    ")\n",
    "\n",
    "print(\"TinyLlama loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chat with TinyLlama using the messages format\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Explain machine learning in simple terms.\"}\n",
    "]\n",
    "\n",
    "result = chat(\n",
    "    messages,\n",
    "    max_new_tokens=150,\n",
    "    do_sample=True,\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "print(result[0][\"generated_text\"][-1][\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try another question\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What are 3 tips for learning to code?\"}\n",
    "]\n",
    "\n",
    "result = chat(\n",
    "    messages,\n",
    "    max_new_tokens=200,\n",
    "    do_sample=True,\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "print(result[0][\"generated_text\"][-1][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Exercise\n",
    "\n",
    "Experiment with generation parameters! Change the `temperature` and `max_new_tokens` below and observe how the output changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise: Change the parameters and run this cell multiple times\n",
    "# Try: temperature=0.2, temperature=1.5, max_new_tokens=50 vs 200\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Write a short poem about coding.\"}\n",
    "]\n",
    "\n",
    "result = chat(\n",
    "    messages,\n",
    "    max_new_tokens=100,   # Try changing this\n",
    "    do_sample=True,\n",
    "    temperature=0.7       # Try changing this\n",
    ")\n",
    "\n",
    "print(result[0][\"generated_text\"][-1][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Going Further\n",
    "\n",
    "In this notebook we used small models (GPT-2 at 124M and TinyLlama at 1.1B parameters). To run larger, more capable models:\n",
    "\n",
    "- **Quantization** reduces model precision (e.g., from 16-bit to 4-bit), cutting memory usage by 4x. This lets you run 7B parameter models on a consumer GPU. Libraries like `bitsandbytes` make this easy.\n",
    "- **Gated models** (like Llama 3) require a free HuggingFace account and access approval. You authenticate using `huggingface_hub.login()` with a token from [huggingface.co/settings/tokens](https://huggingface.co/settings/tokens).\n",
    "- **Larger GPUs** (A100, H100) or cloud services let you run even bigger models without quantization.\n",
    "\n",
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **AutoModelForCausalLM** loads text generation models; pair it with the matching tokenizer\n",
    "\n",
    "2. **Temperature controls randomness** — low for focused output, high for creative output\n",
    "\n",
    "3. **Chat-style models** (like TinyLlama) follow instructions and can be used with the `pipeline` API and message format\n",
    "\n",
    "---\n",
    "\n",
    "## Additional Resources\n",
    "\n",
    "- [HuggingFace Model Hub](https://huggingface.co/models)\n",
    "- [Text Generation Strategies](https://huggingface.co/docs/transformers/generation_strategies)\n",
    "- [TinyLlama Model Card](https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0)\n",
    "\n",
    "---\n",
    "\n",
    "**Course Information:**\n",
    "- **Institution:** CV Raman Global University, Bhubaneswar\n",
    "- **Program:** AI Center of Excellence\n",
    "- **Course:** AI Systems Engineering 1\n",
    "- **Developed by:** [Poorit Technologies](https://poorit.in) — *Transform Graduates into Industry-Ready Professionals*\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}