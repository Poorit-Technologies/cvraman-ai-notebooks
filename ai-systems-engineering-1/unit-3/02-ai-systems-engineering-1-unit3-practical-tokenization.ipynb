{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "<img src=\"https://poorit.in/image.png\" alt=\"Poorit\" width=\"40\" style=\"vertical-align: middle;\"> <b>AI SYSTEMS ENGINEERING 1</b>\n",
    "\n",
    "## Unit 3: Practical Tokenization\n",
    "\n",
    "**CV Raman Global University, Bhubaneswar**  \n",
    "*AI Center of Excellence*\n",
    "\n",
    "---\n",
    "\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "### What You'll Learn\n",
    "\n",
    "In this notebook, you will:\n",
    "\n",
    "1. **Understand why tokenization matters** — cost, context length, and performance\n",
    "2. **Use HuggingFace tokenizers** to encode and decode text\n",
    "3. **Compare tokenizers** across different models and languages\n",
    "\n",
    "**Duration:** ~30 minutes\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q transformers tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Why Tokenization Matters\n",
    "\n",
    "LLMs don't process raw text — they work with **tokens** (numerical representations).\n",
    "\n",
    "Tokenization affects:\n",
    "- **Cost** — API pricing is per token\n",
    "- **Context length** — models have token limits (e.g., GPT-4 has 128k tokens)\n",
    "- **Performance** — different tokenizers handle languages differently"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Loading Tokenizers\n",
    "\n",
    "Each model has its own tokenizer trained on specific data. Let's load two popular ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizers for GPT-2 and BERT\n",
    "gpt2_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check vocabulary sizes\n",
    "print(f\"GPT-2 vocabulary size: {len(gpt2_tokenizer):,}\")\n",
    "print(f\"BERT vocabulary size:  {len(bert_tokenizer):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Basic Encoding and Decoding\n",
    "\n",
    "**Encoding** converts text to token IDs. **Decoding** converts token IDs back to text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Hello, I am studying at CV Raman University!\"\n",
    "\n",
    "# Encode text to token IDs\n",
    "tokens = gpt2_tokenizer.encode(text)\n",
    "print(f\"Text: {text}\")\n",
    "print(f\"Token IDs: {tokens}\")\n",
    "print(f\"Number of tokens: {len(tokens)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decode back to text\n",
    "decoded = gpt2_tokenizer.decode(tokens)\n",
    "print(f\"Decoded: {decoded}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See individual tokens\n",
    "token_strings = gpt2_tokenizer.convert_ids_to_tokens(tokens)\n",
    "\n",
    "print(\"Token breakdown:\")\n",
    "for token_id, token_str in zip(tokens, token_strings):\n",
    "    print(f\"  {token_id:6d} → '{token_str}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how some words are split into sub-words (e.g., `Ġstudying` — the `Ġ` represents a leading space). This is how the tokenizer handles words it hasn't seen as a whole."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Comparing Tokenizers\n",
    "\n",
    "Different models tokenize the same text differently. Let's compare GPT-2 and BERT on English, Hindi, and code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_tokenizers(text, tokenizers):\n",
    "    \"\"\"Compare how different tokenizers handle the same text.\"\"\"\n",
    "    print(f\"Text: {text}\\n\")\n",
    "\n",
    "    for name, tokenizer in tokenizers.items():\n",
    "        tokens = tokenizer.encode(text)\n",
    "        token_strings = tokenizer.convert_ids_to_tokens(tokens)\n",
    "\n",
    "        print(f\"{name}:\")\n",
    "        print(f\"  Tokens: {len(tokens)}\")\n",
    "        print(f\"  Breakdown: {token_strings}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizers = {\n",
    "    \"GPT-2\": gpt2_tokenizer,\n",
    "    \"BERT\": bert_tokenizer\n",
    "}\n",
    "\n",
    "# English text\n",
    "compare_tokenizers(\"Artificial Intelligence is transforming industries.\", tokenizers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hindi text — notice how many more tokens are needed\n",
    "compare_tokenizers(\"भारत एक महान देश है।\", tokenizers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code — tokenizers handle code differently too\n",
    "compare_tokenizers(\"def hello_world(): print('Hello!')\", tokenizers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key observations:**\n",
    "- English text is tokenized efficiently by both models\n",
    "- Hindi text requires significantly more tokens — these tokenizers were trained mostly on English\n",
    "- Code tokenization varies; GPT-2 handles it slightly better since it was trained on web text that includes code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### A Note on Tokenization Strategies\n",
    "\n",
    "Different models use different algorithms to build their vocabulary:\n",
    "\n",
    "| Strategy | Used By | Key Idea |\n",
    "|----------|---------|----------|\n",
    "| **BPE** (Byte Pair Encoding) | GPT-2, GPT-4 | Starts with characters, repeatedly merges the most frequent pairs |\n",
    "| **WordPiece** | BERT | Similar to BPE, but uses likelihood to decide merges |\n",
    "| **SentencePiece** | Llama, T5 | Language-agnostic, works directly on raw text (no pre-tokenization) |\n",
    "\n",
    "All three approaches learn sub-word vocabularies — they split rare words into pieces while keeping common words whole. You don't need to choose a strategy yourself; it's baked into each model's tokenizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Exercise\n",
    "\n",
    "Pick 3 sentences — one in English, one in Hindi (or another non-English language), and one code snippet. Tokenize each with GPT-2 and count the tokens. Which type of content is the most token-efficient?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise: Token counting\n",
    "# Replace the strings below with your own text\n",
    "\n",
    "english_text = \"\"  # e.g. \"The quick brown fox jumps over the lazy dog.\"\n",
    "hindi_text = \"\"    # e.g. \"नमस्ते, आप कैसे हैं?\"\n",
    "code_text = \"\"     # e.g. \"for i in range(10): print(i)\"\n",
    "\n",
    "for label, text in [(\"English\", english_text), (\"Hindi\", hindi_text), (\"Code\", code_text)]:\n",
    "    tokens = gpt2_tokenizer.encode(text)\n",
    "    print(f\"{label}: {len(tokens)} tokens for {len(text)} characters (ratio: {len(text)/len(tokens):.1f} chars/token)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **Tokenizers are model-specific** — always use the matching tokenizer for a model\n",
    "\n",
    "2. **Token counts vary by content** — non-English text and code often use more tokens\n",
    "\n",
    "3. **Sub-word tokenization** — rare words are split into pieces, common words stay whole\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "In the next notebook, we'll go beyond pipelines and **load and run open-source LLMs** directly, exploring generation parameters like temperature and sampling.\n",
    "\n",
    "---\n",
    "\n",
    "## Additional Resources\n",
    "\n",
    "- [Tokenizers Documentation](https://huggingface.co/docs/tokenizers)\n",
    "- [Understanding Tokenization](https://huggingface.co/docs/transformers/tokenizer_summary)\n",
    "\n",
    "---\n",
    "\n",
    "**Course Information:**\n",
    "- **Institution:** CV Raman Global University, Bhubaneswar\n",
    "- **Program:** AI Center of Excellence\n",
    "- **Course:** AI Systems Engineering 1\n",
    "- **Developed by:** [Poorit Technologies](https://poorit.in) — *Transform Graduates into Industry-Ready Professionals*\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
