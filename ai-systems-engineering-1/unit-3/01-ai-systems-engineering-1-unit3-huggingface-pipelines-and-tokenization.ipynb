{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "<img src=\"https://poorit.in/image.png\" alt=\"Poorit\" width=\"40\" style=\"vertical-align: middle;\"> <b>AI SYSTEMS ENGINEERING 1</b>\n",
    "\n",
    "## Unit 3: HuggingFace Pipelines & Tokenization\n",
    "\n",
    "**CV Raman Global University, Bhubaneswar**  \n",
    "*AI Center of Excellence*\n",
    "\n",
    "---\n",
    "\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "### What You'll Learn\n",
    "\n",
    "In this notebook, you will:\n",
    "\n",
    "1. **Understand the HuggingFace ecosystem** — Hub, Transformers, Datasets, Spaces\n",
    "2. **Use the Pipelines API** for text generation and sentiment analysis\n",
    "3. **Understand why tokenization matters** — cost, context length, and performance\n",
    "4. **Use HuggingFace tokenizers** to encode and decode text\n",
    "5. **Compare tokenizers** across different models and languages\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers torch tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import pipeline, AutoTokenizer\n",
    "\n",
    "device = 0 if torch.cuda.is_available() else -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. The HuggingFace Ecosystem\n",
    "\n",
    "HuggingFace is the largest platform for sharing and using AI models.\n",
    "\n",
    "| Component | Description |\n",
    "|-----------|-------------|\n",
    "| **Hub** | Repository of 500k+ models and datasets |\n",
    "| **Transformers** | Library for working with transformer models |\n",
    "| **Datasets** | Library for loading and processing datasets |\n",
    "| **Spaces** | Platform for hosting ML demos |\n",
    "\n",
    "The **Pipelines API** is the simplest way to use models — it handles tokenization, inference, and post-processing automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Text Generation Pipeline\n",
    "\n",
    "Let's start with text generation using GPT-2, a small but capable model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = pipeline(\"text-generation\", model=\"gpt2\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Artificial intelligence is transforming\"\n",
    "\n",
    "result = generator(prompt, max_new_tokens=50, do_sample=True, temperature=0.7)\n",
    "\n",
    "print(result[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it — one line to load the model, one call to generate text. The pipeline handles tokenization, model inference, and decoding automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Sentiment Analysis Pipeline\n",
    "\n",
    "Sentiment analysis classifies text as positive or negative. The pipeline uses a pretrained model (DistilBERT fine-tuned on SST-2) by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment = pipeline(\"sentiment-analysis\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\n",
    "    \"I love this course! It's really helping me understand AI.\",\n",
    "    \"The weather today is terrible and I'm stuck indoors.\",\n",
    "    \"The food was okay, nothing special.\"\n",
    "]\n",
    "\n",
    "results = sentiment(texts)\n",
    "\n",
    "for text, result in zip(texts, results):\n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"Sentiment: {result['label']} (confidence: {result['score']:.2f})\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Why Tokenization Matters\n",
    "\n",
    "LLMs don't process raw text — they work with **tokens** (numerical representations).\n",
    "\n",
    "Tokenization affects:\n",
    "- **Cost** — API pricing is per token\n",
    "- **Context length** — models have token limits (e.g., GPT-4 has 128k tokens)\n",
    "- **Performance** — different tokenizers handle languages differently"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Basic Encoding & Decoding\n",
    "\n",
    "**Encoding** converts text to token IDs. **Decoding** converts token IDs back to text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Hello, I am studying at CV Raman University!\"\n",
    "\n",
    "tokens = gpt2_tokenizer.encode(text)\n",
    "print(f\"Text: {text}\")\n",
    "print(f\"Token IDs: {tokens}\")\n",
    "print(f\"Number of tokens: {len(tokens)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded = gpt2_tokenizer.decode(tokens)\n",
    "print(f\"Decoded: {decoded}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_strings = gpt2_tokenizer.convert_ids_to_tokens(tokens)\n",
    "\n",
    "print(\"Token breakdown:\")\n",
    "for token_id, token_str in zip(tokens, token_strings):\n",
    "    print(f\"  {token_id:6d} → '{token_str}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how some words are split into sub-words (e.g., `Ġstudying` — the `Ġ` represents a leading space). This is how the tokenizer handles words it hasn't seen as a whole."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Comparing Tokenizers\n",
    "\n",
    "Different models tokenize the same text differently. Let's compare GPT-2 and BERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "print(f\"GPT-2 vocabulary size: {len(gpt2_tokenizer):,}\")\n",
    "print(f\"BERT vocabulary size:  {len(bert_tokenizer):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_tokenizers(text, tokenizers):\n",
    "    \"\"\"Compare how different tokenizers handle the same text.\"\"\"\n",
    "    print(f\"Text: {text}\\n\")\n",
    "\n",
    "    for name, tokenizer in tokenizers.items():\n",
    "        tokens = tokenizer.encode(text)\n",
    "        token_strings = tokenizer.convert_ids_to_tokens(tokens)\n",
    "\n",
    "        print(f\"{name}:\")\n",
    "        print(f\"  Tokens: {len(tokens)}\")\n",
    "        print(f\"  Breakdown: {token_strings}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizers = {\n",
    "    \"GPT-2\": gpt2_tokenizer,\n",
    "    \"BERT\": bert_tokenizer\n",
    "}\n",
    "\n",
    "compare_tokenizers(\"Artificial Intelligence is transforming industries.\", tokenizers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_tokenizers(\"भारत एक महान देश है।\", tokenizers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key observations:**\n",
    "- English text is tokenized efficiently by both models\n",
    "- Hindi text requires significantly more tokens — these tokenizers were trained mostly on English"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Tokenization Strategies\n",
    "\n",
    "Different models use different algorithms to build their vocabulary:\n",
    "\n",
    "| Strategy | Used By | Key Idea |\n",
    "|----------|---------|----------|\n",
    "| **BPE** (Byte Pair Encoding) | GPT-2, GPT-4 | Starts with characters, repeatedly merges the most frequent pairs |\n",
    "| **WordPiece** | BERT | Similar to BPE, but uses likelihood to decide merges |\n",
    "| **SentencePiece** | Llama, T5 | Language-agnostic, works directly on raw text (no pre-tokenization) |\n",
    "\n",
    "All three approaches learn sub-word vocabularies — they split rare words into pieces while keeping common words whole. You don't need to choose a strategy yourself; it's baked into each model's tokenizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Exercise\n",
    "\n",
    "**Step 1:** Run sentiment analysis on 3 sentences of your choice.  \n",
    "**Step 2:** Tokenize each sentence with GPT-2 and count the tokens. Which is the most token-efficient?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Sentiment analysis on your own sentences\n",
    "\n",
    "my_texts = [\n",
    "    # \"your first sentence here\",\n",
    "    # \"your second sentence here\",\n",
    "    # \"your third sentence here\",\n",
    "]\n",
    "\n",
    "results = sentiment(my_texts)\n",
    "\n",
    "for text, result in zip(my_texts, results):\n",
    "    print(f\"{text}\")\n",
    "    print(f\"  → {result['label']} ({result['score']:.2f})\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Count tokens for each sentence\n",
    "\n",
    "for text in my_texts:\n",
    "    tokens = gpt2_tokenizer.encode(text)\n",
    "    print(f\"{text}\")\n",
    "    print(f\"  → {len(tokens)} tokens ({len(text)/len(tokens):.1f} chars/token)\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Key Takeaways\n\n1. **Pipelines abstract complexity** — tokenization, inference, and post-processing are handled automatically\n\n2. **Many tasks supported** — text generation, classification, NER, zero-shot classification, and more\n\n3. **Tokenizers are model-specific** — always use the matching tokenizer for a model\n\n4. **Token counts vary by content** — non-English text often uses more tokens\n\n5. **Sub-word tokenization** — rare words are split into pieces, common words stay whole\n\n### Common Pipeline Tasks\n\n| Task | Pipeline Name | Example Model |\n|------|--------------|---------------|\n| Text Generation | `text-generation` | gpt2, llama |\n| Classification | `sentiment-analysis` | distilbert |\n| Named Entities | `ner` | bert |\n| Zero-Shot Classification | `zero-shot-classification` | bart-large-mnli |\n\n### What's Next?\n\nIn the next notebook, we'll **build a multi-tool text analyzer app** using Gradio and the HuggingFace pipelines you learned here — sentiment analysis, named entity recognition, zero-shot classification, and token counting, all in one interface.\n\n---\n\n## Additional Resources\n\n- [HuggingFace Hub](https://huggingface.co/models)\n- [Transformers Documentation](https://huggingface.co/docs/transformers)\n- [Pipeline Tutorial](https://huggingface.co/docs/transformers/pipeline_tutorial)\n- [Tokenizers Documentation](https://huggingface.co/docs/tokenizers)\n- [Understanding Tokenization](https://huggingface.co/docs/transformers/tokenizer_summary)\n\n---\n\n**Course Information:**\n- **Institution:** CV Raman Global University, Bhubaneswar\n- **Program:** AI Center of Excellence\n- **Course:** AI Systems Engineering 1\n- **Developed by:** [Poorit Technologies](https://poorit.in) — *Transform Graduates into Industry-Ready Professionals*\n\n---"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}