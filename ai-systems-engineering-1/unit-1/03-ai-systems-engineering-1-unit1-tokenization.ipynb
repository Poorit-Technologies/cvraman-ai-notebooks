{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<div align=\"center\">\n<img src=\"https://poorit.in/image.png\" alt=\"Poorit\" width=\"40\" style=\"vertical-align: middle;\"> <b>AI SYSTEMS ENGINEERING 1</b>\n\n## Unit 1: Tokenization and Conversation Memory\n\n**CV Raman Global University, Bhubaneswar**  \n*AI Center of Excellence*\n\n</div>\n\n---\n\n### What You'll Learn\n\nIn this notebook, you will:\n\n1. **Understand tokenization** and how text is converted to tokens\n2. **Use tiktoken** to encode and decode text\n3. **Learn about context windows** and their implications for API costs\n4. **Understand conversation memory** and the \"illusion\" of memory in LLMs\n\n**Duration:** ~1 hour\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q openai tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "import tiktoken\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure OpenAI\n",
    "api_key = getpass(\"Enter your OpenAI API Key: \")\n",
    "os.environ['OPENAI_API_KEY'] = api_key\n",
    "client = OpenAI(api_key=api_key)\n",
    "MODEL = \"gpt-4o-mini\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. What is Tokenization?\n",
    "\n",
    "**Tokens** are the fundamental units that LLMs work with. They're not exactly words or characters, but somewhere in between.\n",
    "\n",
    "- A token is typically 3-4 characters\n",
    "- Common words are often single tokens\n",
    "- Rare words may be split into multiple tokens\n",
    "\n",
    "**Why does this matter?**\n",
    "- API pricing is based on tokens (input + output)\n",
    "- Context windows are measured in tokens\n",
    "- Understanding tokens helps optimize costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the tokenizer for GPT-4o-mini\n",
    "encoding = tiktoken.encoding_for_model(\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode a sentence into tokens\n",
    "text = \"Hello, my name is Ravi and I study at CV Raman University\"\n",
    "tokens = encoding.encode(text)\n",
    "\n",
    "print(f\"Text: {text}\")\n",
    "print(f\"Token count: {len(tokens)}\")\n",
    "print(f\"Token IDs: {tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See what each token represents\n",
    "for token_id in tokens:\n",
    "    token_text = encoding.decode([token_id])\n",
    "    print(f\"{token_id:6d} ‚Üí '{token_text}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decode tokens back to text\n",
    "decoded_text = encoding.decode(tokens)\n",
    "print(f\"Decoded: {decoded_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Token Counting and Cost Estimation\n",
    "\n",
    "Let's create a utility to count tokens and estimate API costs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT-4o-mini pricing (as of 2024)\n",
    "INPUT_PRICE_PER_1M = 0.15  # $0.15 per 1M input tokens\n",
    "OUTPUT_PRICE_PER_1M = 0.60  # $0.60 per 1M output tokens\n",
    "\n",
    "def count_tokens(text):\n",
    "    \"\"\"Count the number of tokens in a text.\"\"\"\n",
    "    return len(encoding.encode(text))\n",
    "\n",
    "def estimate_cost(input_tokens, output_tokens):\n",
    "    \"\"\"Estimate cost in USD.\"\"\"\n",
    "    input_cost = (input_tokens / 1_000_000) * INPUT_PRICE_PER_1M\n",
    "    output_cost = (output_tokens / 1_000_000) * OUTPUT_PRICE_PER_1M\n",
    "    return input_cost + output_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Estimate cost for a conversation\n",
    "system_prompt = \"You are a helpful assistant that explains concepts clearly.\"\n",
    "user_message = \"Explain machine learning in simple terms.\"\n",
    "\n",
    "input_tokens = count_tokens(system_prompt + user_message)\n",
    "estimated_output = 200  # Assume ~200 tokens output\n",
    "\n",
    "print(f\"Input tokens: {input_tokens}\")\n",
    "print(f\"Estimated output: {estimated_output}\")\n",
    "print(f\"Estimated cost: ${estimate_cost(input_tokens, estimated_output):.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Context Windows\n",
    "\n",
    "Every model has a **context window** - the maximum number of tokens it can process in a single request.\n",
    "\n",
    "| Model | Context Window |\n",
    "|-------|---------------|\n",
    "| GPT-4o-mini | 128,000 tokens |\n",
    "| GPT-4o | 128,000 tokens |\n",
    "| Claude 3.5 Sonnet | 200,000 tokens |\n",
    "| Llama 3.2 | 128,000 tokens |\n",
    "\n",
    "**Important**: Context window includes both input AND output tokens!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check how much of context window a large text would use\n",
    "large_text = \"This is a test sentence. \" * 1000\n",
    "token_count = count_tokens(large_text)\n",
    "\n",
    "context_window = 128_000\n",
    "usage_percent = (token_count / context_window) * 100\n",
    "\n",
    "print(f\"Text length: {len(large_text)} characters\")\n",
    "print(f\"Token count: {token_count}\")\n",
    "print(f\"Context window usage: {usage_percent:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. The \"Illusion\" of Memory\n",
    "\n",
    "Here's an important insight: **LLMs have no memory**. Every API call is completely stateless.\n",
    "\n",
    "Let's demonstrate this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First message - introduce ourselves\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "    {\"role\": \"user\", \"content\": \"Hi! I'm Priya!\"}\n",
    "]\n",
    "\n",
    "response = client.chat.completions.create(model=MODEL, messages=messages)\n",
    "print(\"Response:\", response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New message - ask for our name (without context)\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "    {\"role\": \"user\", \"content\": \"What's my name?\"}\n",
    "]\n",
    "\n",
    "response = client.chat.completions.create(model=MODEL, messages=messages)\n",
    "print(\"Response:\", response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The LLM doesn't remember!\n",
    "\n",
    "Every call is stateless. To create the \"illusion\" of memory, we must include the full conversation history:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Include the full conversation history\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "    {\"role\": \"user\", \"content\": \"Hi! I'm Priya!\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Hello Priya! It's nice to meet you. How can I help you today?\"},\n",
    "    {\"role\": \"user\", \"content\": \"What's my name?\"}\n",
    "]\n",
    "\n",
    "response = client.chat.completions.create(model=MODEL, messages=messages)\n",
    "print(\"Response:\", response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Insights\n",
    "\n",
    "1. **Every call is stateless** - the model doesn't \"remember\" previous calls\n",
    "2. **We pass the full conversation** - this creates the illusion of memory\n",
    "3. **Cost implications** - longer conversations cost more (more tokens)\n",
    "4. **ChatGPT uses this trick** - it stores and sends the full conversation each time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Building a Conversation Manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conversation:\n",
    "    \"\"\"Manage a conversation with memory.\"\"\"\n",
    "    \n",
    "    def __init__(self, system_prompt=\"You are a helpful assistant\"):\n",
    "        self.messages = [{\"role\": \"system\", \"content\": system_prompt}]\n",
    "    \n",
    "    def chat(self, user_message):\n",
    "        self.messages.append({\"role\": \"user\", \"content\": user_message})\n",
    "        \n",
    "        response = client.chat.completions.create(\n",
    "            model=MODEL,\n",
    "            messages=self.messages\n",
    "        )\n",
    "        \n",
    "        assistant_message = response.choices[0].message.content\n",
    "        self.messages.append({\"role\": \"assistant\", \"content\": assistant_message})\n",
    "        \n",
    "        return assistant_message\n",
    "    \n",
    "    def get_token_count(self):\n",
    "        total = sum(count_tokens(m[\"content\"]) for m in self.messages)\n",
    "        return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the conversation manager\n",
    "conv = Conversation()\n",
    "\n",
    "print(\"User: Hi, I'm Amit!\")\n",
    "print(\"Assistant:\", conv.chat(\"Hi, I'm Amit!\"))\n",
    "\n",
    "print(\"\\nUser: What's my name?\")\n",
    "print(\"Assistant:\", conv.chat(\"What's my name?\"))\n",
    "\n",
    "print(f\"\\nTotal tokens used: {conv.get_token_count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Exercise: Token Analysis\n",
    "\n",
    "Analyze how different languages and text types tokenize differently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise: Compare token counts for different texts\n",
    "texts = [\n",
    "    \"Hello world\",\n",
    "    \"Artificial Intelligence\",\n",
    "    \"‡§®‡§Æ‡§∏‡•ç‡§§‡•á\",  # Hindi\n",
    "    \"ü§ñüí°üöÄ\",  # Emojis\n",
    "]\n",
    "\n",
    "for text in texts:\n",
    "    tokens = encoding.encode(text)\n",
    "    print(f\"'{text}' ‚Üí {len(tokens)} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **Tokens** are the units LLMs work with - typically 3-4 characters\n",
    "\n",
    "2. **tiktoken** is OpenAI's tokenizer library - use it to count tokens and estimate costs\n",
    "\n",
    "3. **Context windows** limit how much text you can process - includes input AND output\n",
    "\n",
    "4. **LLMs are stateless** - memory is created by passing the full conversation each time\n",
    "\n",
    "5. **Cost optimization** - manage conversation length to control API costs\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "In the next notebook, we'll explore:\n",
    "- JSON structured outputs\n",
    "- Chaining multiple LLM calls\n",
    "- Streaming responses for better UX\n",
    "\n",
    "---\n",
    "\n",
    "## Additional Resources\n",
    "\n",
    "- [OpenAI Tokenizer Tool](https://platform.openai.com/tokenizer)\n",
    "- [tiktoken Documentation](https://github.com/openai/tiktoken)\n",
    "- [OpenAI Pricing](https://openai.com/pricing)\n",
    "\n",
    "---\n",
    "\n",
    "**Course Information:**\n",
    "- **Institution:** CV Raman Global University, Bhubaneswar\n",
    "- **Program:** AI Center of Excellence\n",
    "- **Course:** AI Systems Engineering 1\n",
    "- **Developed by:** [Poorit Technologies](https://poorit.in) - *Transform Graduates into Industry-Ready Professionals*\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}