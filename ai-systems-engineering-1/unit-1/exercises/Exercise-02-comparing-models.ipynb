{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "<img src=\"https://poorit.in/image.png\" alt=\"Poorit\" width=\"40\" style=\"vertical-align: middle;\"> <b>AI SYSTEMS ENGINEERING 1</b>\n",
    "\n",
    "## Unit 1 Exercises: Comparing Model Providers\n",
    "\n",
    "**CV Raman Global University, Bhubaneswar**  \n",
    "*AI Center of Excellence*\n",
    "\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "Complete the exercises below using the setup provided. Each question has one or more empty code cells for your solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Run the cells below to install packages, import libraries, and configure API clients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q openai requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import json\n",
    "import requests\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Configure Gemini API Key\nfrom google.colab import userdata\nfrom getpass import getpass\n\n# GEMINI_API_KEY = userdata.get(\"GEMINI_API_KEY\")\nGEMINI_API_KEY = getpass(\"Enter your Gemini API key: \")\nGEMINI_BASE_URL = \"https://generativelanguage.googleapis.com/v1beta/openai/\"\n\ngemini_client = OpenAI(\n    base_url=GEMINI_BASE_URL,\n    api_key=GEMINI_API_KEY\n)\n\nprint(\"Gemini client configured.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install and start Ollama (run this cell and wait for it to complete)\n",
    "!sudo apt-get update -qq && sudo apt-get install -y -qq zstd > /dev/null\n",
    "!curl -fsSL https://ollama.com/install.sh | sh\n",
    "\n",
    "import subprocess\n",
    "import time\n",
    "\n",
    "subprocess.Popen([\"ollama\", \"serve\"], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
    "time.sleep(5)\n",
    "\n",
    "try:\n",
    "    status = requests.get(\"http://localhost:11434\", timeout=5)\n",
    "    print(\"Ollama is running!\")\n",
    "except:\n",
    "    print(\"Ollama failed to start. Try re-running this cell.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull Ollama models\n",
    "!ollama pull llama3.2:1b\n",
    "!ollama pull deepseek-r1:1.5b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure Ollama client\n",
    "OLLAMA_BASE_URL = \"http://localhost:11434/v1\"\n",
    "\n",
    "ollama_client = OpenAI(\n",
    "    base_url=OLLAMA_BASE_URL,\n",
    "    api_key=\"ollama\"  # Ollama doesn't need a real key\n",
    ")\n",
    "\n",
    "print(\"Ollama client configured.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Q1: Raw HTTP API Call\n",
    "\n",
    "Using the `requests` library (**not** the OpenAI client), make a **raw HTTP POST request** to the Gemini OpenAI-compatible endpoint:\n",
    "\n",
    "```\n",
    "https://generativelanguage.googleapis.com/v1beta/openai/chat/completions\n",
    "```\n",
    "\n",
    "Send the prompt **\"What is the capital of India?\"** and:\n",
    "1. Print the **full JSON response**\n",
    "2. Extract and print **just the assistant's message text**\n",
    "\n",
    "**Hints:**\n",
    "- You'll need to set `Authorization: Bearer <key>` and `Content-Type: application/json` headers\n",
    "- The payload structure matches the OpenAI format: `{\"model\": ..., \"messages\": [...]}`\n",
    "- Use `gemini-2.0-flash-lite` as the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Raw HTTP POST request to Gemini endpoint\n\nurl = \"https://generativelanguage.googleapis.com/v1beta/openai/chat/completions\"\n\nheaders = {\n    \"Authorization\": f\"Bearer {___}\",\n    \"Content-Type\": \"application/json\"\n}\n\npayload = {\n    \"model\": \"gemini-2.0-flash-lite\",\n    \"messages\": [\n        {\"role\": \"user\", \"content\": \"___\"}\n    ]\n}\n\nresponse = requests.post(url, headers=headers, json=payload)\n\n# Print full JSON response\ndata = response.json()\nprint(json.dumps(data, indent=2))\n\n# Extract just the assistant's message\nprint(\"\\nAssistant:\", data[\"choices\"][___][\"message\"][\"___\"])"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Q2: Three-Model Comparison\n",
    "\n",
    "Using the OpenAI-compatible endpoint approach, send the prompt **\"Explain the internet in 3 sentences\"** to:\n",
    "\n",
    "1. **Gemini** (`gemini-2.0-flash-lite`)\n",
    "2. **Ollama — Llama** (`llama3.2:1b`)\n",
    "3. **Ollama — DeepSeek** (`deepseek-r1:1.5b`)\n",
    "\n",
    "Display all three responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "prompt = \"Explain the internet in 3 sentences\"\n\n# Define models to compare: (name, client, model_id)\nmodels = [\n    (\"Gemini\",  gemini_client, \"gemini-2.0-flash-lite\"),\n    (\"Llama\",   ollama_client, \"llama3.2:1b\"),\n    (\"DeepSeek\", ollama_client, \"deepseek-r1:1.5b\"),\n]\n\nfor name, api_client, model_id in models:\n    response = api_client.chat.completions.create(\n        model=___,\n        messages=[{\"role\": \"user\", \"content\": ___}]\n    )\n    print(f\"\\n--- {name} ({model_id}) ---\")\n    print(response.choices[0].message.content)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Written Response:** In a short paragraph, compare the three responses: Which was most accurate? Which was easiest to understand? Did any model fail to follow the \"3 sentences\" instruction?\n",
    "\n",
    "*Your answer here:*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Q3: Client Library vs. Raw HTTP\n",
    "\n",
    "For the prompt **\"Tell me a fun fact about Odisha\"**, make the API call **two ways**:\n",
    "\n",
    "1. Using the **OpenAI Python client library** pointed at Gemini\n",
    "2. Using a **raw `requests.post()`** call to Gemini\n",
    "\n",
    "Print both responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Method 1: Using the OpenAI client library pointed at Gemini\nresponse = gemini_client.chat.completions.create(\n    model=\"gemini-2.0-flash-lite\",\n    messages=[{\"role\": \"user\", \"content\": \"___\"}]\n)\nprint(\"Client library response:\")\nprint(response.choices[0].message.content)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Method 2: Raw HTTP request to Gemini\nheaders = {\n    \"Authorization\": f\"Bearer {___}\",\n    \"Content-Type\": \"application/json\"\n}\n\npayload = {\n    \"model\": \"___\",\n    \"messages\": [\n        {\"role\": \"user\", \"content\": \"___\"}\n    ]\n}\n\nresponse = requests.post(\n    GEMINI_BASE_URL + \"chat/completions\",\n    headers=___,\n    json=___\n)\n\nprint(\"Raw HTTP response:\")\nprint(response.json()[\"choices\"][0][\"message\"][\"content\"])"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Written Response:** In 2–3 sentences, explain: Did you get the same response? Which method required more code? Why might you prefer one over the other?\n",
    "\n",
    "*Your answer here:*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Course Information:**\n",
    "- **Institution:** CV Raman Global University, Bhubaneswar\n",
    "- **Program:** AI Center of Excellence\n",
    "- **Course:** AI Systems Engineering 1\n",
    "- **Developed by:** [Poorit Technologies](https://poorit.in) - *Transform Graduates into Industry-Ready Professionals*\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}