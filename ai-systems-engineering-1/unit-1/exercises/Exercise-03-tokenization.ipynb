{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "<img src=\"https://poorit.in/image.png\" alt=\"Poorit\" width=\"40\" style=\"vertical-align: middle;\"> <b>AI SYSTEMS ENGINEERING 1</b>\n",
    "\n",
    "## Unit 1 Exercises: Tokenization and Conversation Memory\n",
    "\n",
    "**CV Raman Global University, Bhubaneswar**  \n",
    "*AI Center of Excellence*\n",
    "\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "Complete the exercises below using the helper functions and setup provided. Each question has one or more empty code cells for your solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Run the cells below to install packages, import libraries, and define helper functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q openai tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import tiktoken\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Configure Gemini API Key\nfrom google.colab import userdata\nfrom getpass import getpass\n\n# GEMINI_API_KEY = userdata.get(\"GEMINI_API_KEY\")\nGEMINI_API_KEY = getpass(\"Enter your Gemini API key: \")\nGEMINI_BASE_URL = \"https://generativelanguage.googleapis.com/v1beta/openai/\"\n\nclient = OpenAI(\n    base_url=GEMINI_BASE_URL,\n    api_key=GEMINI_API_KEY\n)\n\nMODEL = \"gemini-2.0-flash-lite\"\nprint(f\"Gemini configured with model: {MODEL}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer and helper functions\n",
    "\n",
    "encoding = tiktoken.encoding_for_model(\"gpt-4o-mini\")\n",
    "\n",
    "# GPT-4o-mini pricing (as of 2024)\n",
    "INPUT_PRICE_PER_1M = 0.15   # $0.15 per 1M input tokens\n",
    "OUTPUT_PRICE_PER_1M = 0.60  # $0.60 per 1M output tokens\n",
    "\n",
    "\n",
    "def count_tokens(text):\n",
    "    \"\"\"Count the number of tokens in a text.\"\"\"\n",
    "    return len(encoding.encode(text))\n",
    "\n",
    "\n",
    "def estimate_cost(input_tokens, output_tokens):\n",
    "    \"\"\"Estimate cost in USD.\"\"\"\n",
    "    input_cost = (input_tokens / 1_000_000) * INPUT_PRICE_PER_1M\n",
    "    output_cost = (output_tokens / 1_000_000) * OUTPUT_PRICE_PER_1M\n",
    "    return input_cost + output_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversation class for managing multi-turn conversations\n",
    "\n",
    "class Conversation:\n",
    "    \"\"\"Manage a conversation with memory.\"\"\"\n",
    "\n",
    "    def __init__(self, system_prompt=\"You are a helpful assistant\"):\n",
    "        self.messages = [{\"role\": \"system\", \"content\": system_prompt}]\n",
    "\n",
    "    def chat(self, user_message):\n",
    "        self.messages.append({\"role\": \"user\", \"content\": user_message})\n",
    "\n",
    "        response = client.chat.completions.create(\n",
    "            model=MODEL,\n",
    "            messages=self.messages\n",
    "        )\n",
    "\n",
    "        assistant_message = response.choices[0].message.content\n",
    "        self.messages.append({\"role\": \"assistant\", \"content\": assistant_message})\n",
    "\n",
    "        return assistant_message\n",
    "\n",
    "    def get_token_count(self):\n",
    "        total = sum(count_tokens(m[\"content\"]) for m in self.messages)\n",
    "        return total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Q1: Token Cost Estimator\n",
    "\n",
    "Using `tiktoken` and the `estimate_cost()` function, calculate the estimated API cost for the following scenario:\n",
    "\n",
    "- **System prompt:** `\"You are a helpful tutor who explains concepts clearly to college students.\"`\n",
    "- **User message:** The full text of the Indian national anthem (Jana Gana Mana) in English transliteration\n",
    "- **Assume 150 output tokens**\n",
    "\n",
    "Print the input token count, output token count, and total estimated cost in USD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# System prompt\nsystem_prompt = \"You are a helpful tutor who explains concepts clearly to college students.\"\n\n# User message: Indian national anthem in English transliteration\nuser_message = \"\"\"\nJana Gana Mana Adhinayaka Jaya He\nBharata Bhagya Vidhata\nPunjab Sindh Gujarat Maratha\nDravida Utkala Banga\nVindhya Himachala Yamuna Ganga\nUchchala Jaladhi Taranga\nTava Shubha Name Jage\nTava Shubha Ashisha Mange\nGahe Tava Jaya Gatha\nJana Gana Mangala Dayaka Jaya He\nBharata Bhagya Vidhata\nJaya He Jaya He Jaya He\nJaya Jaya Jaya Jaya He\n\"\"\"\n\n# Count input tokens (system + user combined)\ninput_tokens = count_tokens(___) + count_tokens(___)\noutput_tokens = ___  # Given in the question\n\n# Estimate cost\ncost = estimate_cost(___, ___)\n\nprint(f\"Input tokens:  {input_tokens}\")\nprint(f\"Output tokens: {output_tokens}\")\nprint(f\"Estimated cost: ${cost:.6f}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Q2: Context Window Usage\n",
    "\n",
    "Write a function `check_context_fit(text, model_name, context_window)` that:\n",
    "\n",
    "1. Counts the tokens in the given text\n",
    "2. Calculates what percentage of the context window it uses\n",
    "3. Prints whether the text fits within the context window (`True`/`False`)\n",
    "\n",
    "Test it with a string that repeats `\"CV Raman Global University is a great place to study. \"` **500 times**, using GPT-4o-mini's context window of 128,000 tokens.\n",
    "\n",
    "Then increase the repetition count until it **exceeds** the context window. What repetition count is needed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def check_context_fit(text, model_name, context_window):\n    \"\"\"Check if text fits within a model's context window.\"\"\"\n    # Step 1: Count tokens\n    tokens = count_tokens(___)\n\n    # Step 2: Calculate percentage\n    percentage = (___ / ___) * 100\n\n    # Step 3: Check fit\n    fits = tokens ___ context_window  # which comparison operator?\n\n    print(f\"Model: {model_name}\")\n    print(f\"Tokens: {tokens:,}\")\n    print(f\"Context window: {context_window:,}\")\n    print(f\"Usage: {percentage:.2f}%\")\n    print(f\"Fits: {fits}\")\n    return fits"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Test with 500 repetitions\ntext_500 = \"CV Raman Global University is a great place to study. \" * ___\ncheck_context_fit(text_500, \"gpt-4o-mini\", 128_000)\n\n# Now find the repetition count that exceeds the context window\n# Hint: try increasing the multiplier until fits becomes False\nprint(\"\\n--- Finding the limit ---\")\ntext_big = \"CV Raman Global University is a great place to study. \" * ___\ncheck_context_fit(text_big, \"gpt-4o-mini\", 128_000)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Q3: Conversation Memory in Action\n",
    "\n",
    "Using the `Conversation` class, have a conversation with the model where you:\n",
    "\n",
    "1. Tell it: `\"I am studying Computer Science at CV Raman University.\"`\n",
    "2. Tell it: `\"My favorite programming language is Python.\"`\n",
    "3. Ask it: `\"Based on what you know about me, suggest a project idea.\"`\n",
    "4. Print `get_token_count()` after **each** message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create a conversation\nconv = Conversation()\n\n# Message 1\nprint(\"User: I am studying Computer Science at CV Raman University.\")\nprint(\"Assistant:\", conv.chat(\"I am studying Computer Science at CV Raman University.\"))\nprint(f\"Token count: {conv.get_token_count()}\\n\")\n\n# Message 2\nprint(\"User: My favorite programming language is Python.\")\nprint(\"Assistant:\", conv.chat(\"___\"))\nprint(f\"Token count: {conv.get_token_count()}\\n\")\n\n# Message 3\nprint(\"User: Based on what you know about me, suggest a project idea.\")\nprint(\"Assistant:\", conv.chat(\"___\"))\nprint(f\"Token count: {conv.get_token_count()}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Written Response:** Does the token count grow? Explain in 2–3 sentences why this happens and what it means for the cost of long conversations.\n",
    "\n",
    "*Your answer here:*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Q4: Stateless Proof\n",
    "\n",
    "**Without** using the `Conversation` class, make two **separate** API calls:\n",
    "\n",
    "1. First call: `\"My name is [your name] and I study [your branch].\"`\n",
    "2. Second call: `\"What is my name and what do I study?\"`\n",
    "\n",
    "Show that the second call **cannot** answer the question.\n",
    "\n",
    "Then make a **third** call where you manually include both messages in the `messages` list, and show that now it **can** answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Call 1: Introduce yourself (replace the placeholders with your info)\nmessages_1 = [\n    {\"role\": \"user\", \"content\": \"My name is ___ and I study ___.\"}\n]\n\nresponse_1 = client.chat.completions.create(model=MODEL, messages=___)\nprint(\"Call 1 response:\", response_1.choices[0].message.content)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Call 2: Ask without any context — a brand new messages list\nmessages_2 = [\n    {\"role\": \"user\", \"content\": \"What is my name and what do I study?\"}\n]\n\nresponse_2 = client.chat.completions.create(model=___, messages=___)\nprint(\"Call 2 response:\", response_2.choices[0].message.content)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Call 3: Include the full conversation history so the model has context\nmessages_3 = [\n    {\"role\": \"user\", \"content\": \"My name is ___ and I study ___.\"},\n    {\"role\": \"assistant\", \"content\": \"___\"},  # Copy response_1 text here\n    {\"role\": \"user\", \"content\": \"What is my name and what do I study?\"}\n]\n\nresponse_3 = client.chat.completions.create(model=___, messages=___)\nprint(\"Call 3 response:\", response_3.choices[0].message.content)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Written Response:** Explain in 2–3 sentences why the second call cannot answer the question but the third call can.\n",
    "\n",
    "*Your answer here:*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Course Information:**\n",
    "- **Institution:** CV Raman Global University, Bhubaneswar\n",
    "- **Program:** AI Center of Excellence\n",
    "- **Course:** AI Systems Engineering 1\n",
    "- **Developed by:** [Poorit Technologies](https://poorit.in) - *Transform Graduates into Industry-Ready Professionals*\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}