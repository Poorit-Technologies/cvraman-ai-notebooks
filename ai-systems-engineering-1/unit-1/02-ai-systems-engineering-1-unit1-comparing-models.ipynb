{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<div align=\"center\">\n<img src=\"https://poorit.in/image.png\" alt=\"Poorit\" width=\"40\" style=\"vertical-align: middle;\"> <b>AI SYSTEMS ENGINEERING 1</b>\n\n## Unit 1: Comparing Model Providers - OpenAI, Gemini, and Ollama\n\n**CV Raman Global University, Bhubaneswar**  \n*AI Center of Excellence*\n\n---\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Poorit-Technologies/cvraman-ai-notebooks/blob/main/ai-systems-engineering-1/unit-1/02-ai-systems-engineering-1-unit1-comparing-models.ipynb)\n\n</div>\n\n---\n\n### What You'll Learn\n\nIn this notebook, you will:\n\n1. **Understand the Chat Completions API** and how HTTP endpoints work\n2. **Compare Python client libraries** vs raw HTTP requests\n3. **Use OpenAI-compatible endpoints** to connect to Google Gemini\n4. **Run local models with Ollama** for free, private inference\n\n**Duration:** ~1.5 hours\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q openai requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from getpass import getpass\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure OpenAI API Key\n",
    "openai_api_key = getpass(\"Enter your OpenAI API Key: \")\n",
    "os.environ['OPENAI_API_KEY'] = openai_api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Understanding HTTP Endpoints\n",
    "\n",
    "The **Chat Completions API** is simply an HTTP endpoint that:\n",
    "- Receives a POST request with messages\n",
    "- Returns a completion (the model's response)\n",
    "\n",
    "Let's make a raw HTTP call to understand what's happening behind the scenes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Raw HTTP call to OpenAI endpoint\n",
    "\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {openai_api_key}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "payload = {\n",
    "    \"model\": \"gpt-4o-mini\",\n",
    "    \"messages\": [\n",
    "        {\"role\": \"user\", \"content\": \"Tell me a fun fact about India\"}\n",
    "    ]\n",
    "}\n",
    "\n",
    "response = requests.post(\n",
    "    \"https://api.openai.com/v1/chat/completions\",\n",
    "    headers=headers,\n",
    "    json=payload\n",
    ")\n",
    "\n",
    "print(response.json()[\"choices\"][0][\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Python Client Libraries\n",
    "\n",
    "The **openai** package is a Python client library - a wrapper around the HTTP endpoint.\n",
    "\n",
    "It provides:\n",
    "- Cleaner Python syntax\n",
    "- Type hints and autocomplete\n",
    "- Automatic error handling\n",
    "\n",
    "**Important**: The client library is open-source and lightweight. It doesn't contain any model code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the OpenAI client library (much cleaner!)\n",
    "\n",
    "client = OpenAI(api_key=openai_api_key)\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Tell me a fun fact about India\"}]\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. OpenAI-Compatible Endpoints\n",
    "\n",
    "OpenAI's Chat Completions API became so popular that other providers created **compatible endpoints**.\n",
    "\n",
    "This means you can use the same Python client library to call:\n",
    "- OpenAI (GPT models)\n",
    "- Google Gemini\n",
    "- Ollama (local models)\n",
    "- Many others!\n",
    "\n",
    "### Google Gemini\n",
    "\n",
    "To use Gemini, get your API key from: https://aistudio.google.com/apikey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure Gemini (optional)\n",
    "google_api_key = getpass(\"Enter your Google API Key (or press Enter to skip): \")\n",
    "\n",
    "if google_api_key:\n",
    "    GEMINI_BASE_URL = \"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    "    \n",
    "    gemini_client = OpenAI(\n",
    "        base_url=GEMINI_BASE_URL,\n",
    "        api_key=google_api_key\n",
    "    )\n",
    "    \n",
    "    response = gemini_client.chat.completions.create(\n",
    "        model=\"gemini-1.5-flash\",\n",
    "        messages=[{\"role\": \"user\", \"content\": \"Tell me a fun fact about India\"}]\n",
    "    )\n",
    "    \n",
    "    print(\"Gemini response:\")\n",
    "    print(response.choices[0].message.content)\n",
    "else:\n",
    "    print(\"Skipping Gemini - no API key provided\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## 5. Running Local Models with Ollama\n\n**Ollama** lets you run open-source models — and in Google Colab, we can install and run it directly inside the VM!\n\n**Benefits:**\n- Free (no API charges)\n- Private (data stays on the Colab VM)\n- Uses Colab's free T4 GPU (16GB VRAM)\n\n**Limitations:**\n- Less powerful than frontier models\n- Free tier GPU memory limits you to smaller models (1B–3B parameters)\n\nWe'll install Ollama, start the server, and pull a small model — all within this notebook."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Install zstd (required by Ollama installer) and then install Ollama\n!sudo apt-get update -qq && sudo apt-get install -y -qq zstd > /dev/null\n!curl -fsSL https://ollama.com/install.sh | sh\n\n# Start Ollama server as a background daemon\nimport subprocess\nimport time\n\nsubprocess.Popen([\"ollama\", \"serve\"], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\ntime.sleep(5)  # Wait for the server to start\n\n# Verify it's running\ntry:\n    status = requests.get(\"http://localhost:11434\", timeout=5)\n    print(\"Ollama is running!\")\nexcept:\n    print(\"Ollama failed to start. Try re-running this cell.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Pull a small model (~600MB, works well on Colab free tier)\n# For a more capable model, try: !ollama pull llama3.2:3b (requires GPU runtime)\n!ollama pull llama3.2:1b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Connect to Ollama using OpenAI-compatible endpoint\n\nOLLAMA_BASE_URL = \"http://localhost:11434/v1\"\n\nollama_client = OpenAI(\n    base_url=OLLAMA_BASE_URL,\n    api_key=\"ollama\"  # Ollama doesn't need a real key\n)\n\nresponse = ollama_client.chat.completions.create(\n    model=\"llama3.2:1b\",\n    messages=[{\"role\": \"user\", \"content\": \"Tell me a fun fact about India\"}]\n)\n\nprint(\"Llama 3.2 response:\")\nprint(response.choices[0].message.content)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try DeepSeek R1 (reasoning model)\n",
    "!ollama pull deepseek-r1:1.5b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = ollama_client.chat.completions.create(\n",
    "    model=\"deepseek-r1:1.5b\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"What is 15 * 23?\"}]\n",
    ")\n",
    "\n",
    "print(\"DeepSeek R1 response:\")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Comparing Providers\n",
    "\n",
    "Let's create a utility to compare responses from different providers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_models(prompt, clients):\n",
    "    \"\"\"Compare responses from multiple model providers.\"\"\"\n",
    "    for name, (client, model) in clients.items():\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=model,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "            )\n",
    "            print(f\"\\n--- {name} ({model}) ---\")\n",
    "            print(response.choices[0].message.content[:300] + \"...\")\n",
    "        except Exception as e:\n",
    "            print(f\"\\n--- {name} ---\")\n",
    "            print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Set up clients dictionary\nclients = {\n    \"OpenAI\": (client, \"gpt-4o-mini\"),\n    \"Ollama\": (ollama_client, \"llama3.2:1b\")\n}\n\n# Add Gemini if available\nif google_api_key:\n    clients[\"Gemini\"] = (gemini_client, \"gemini-1.5-flash\")\n\n# Compare!\ncompare_models(\"Explain quantum computing in simple terms\", clients)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Exercise: Build a Multi-Provider Summarizer\n",
    "\n",
    "Modify the website summarizer from notebook 01 to use Ollama instead of OpenAI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise: Create a summarizer using Ollama\n",
    "\n",
    "def summarize_with_ollama(text):\n",
    "    \"\"\"Summarize text using a local Ollama model.\"\"\"\n",
    "    # Your implementation here\n",
    "    pass\n",
    "\n",
    "# Test with some sample text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Key Takeaways\n\n1. **Chat Completions API** is an HTTP endpoint - you can call it with raw requests\n\n2. **Python client libraries** are convenient wrappers, not model implementations\n\n3. **OpenAI-compatible endpoints** let you use the same code for multiple providers\n\n4. **Ollama** provides free, private inference with open-source models — and runs inside Google Colab!\n\n### Provider Comparison\n\n| Provider | Cost | Privacy | Quality | Speed |\n|----------|------|---------|---------|-------|\n| OpenAI | Paid | Cloud | Highest | Fast |\n| Gemini | Free tier | Cloud | High | Fast |\n| Ollama | Free | Colab VM | Varies | Depends on hardware |\n\n### What's Next?\n\nIn the next notebook, we'll explore:\n- Tokenization and how text is converted to tokens\n- Understanding context windows and API costs\n- Conversation memory\n\n---\n\n## Additional Resources\n\n- [OpenAI API Documentation](https://platform.openai.com/docs/)\n- [Google AI Studio](https://aistudio.google.com/)\n- [Ollama Documentation](https://ollama.com/)\n\n---\n\n**Course Information:**\n- **Institution:** CV Raman Global University, Bhubaneswar\n- **Program:** AI Center of Excellence\n- **Course:** AI Systems Engineering 1\n- **Developed by:** [Poorit Technologies](https://poorit.in) - *Transform Graduates into Industry-Ready Professionals*\n\n---"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}