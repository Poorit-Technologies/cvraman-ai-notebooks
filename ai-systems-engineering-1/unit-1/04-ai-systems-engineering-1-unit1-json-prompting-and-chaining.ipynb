{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<div align=\"center\">\n<img src=\"https://poorit.in/image.png\" alt=\"Poorit\" width=\"40\" style=\"vertical-align: middle;\"> <b>AI SYSTEMS ENGINEERING 1</b>\n\n## Unit 1: JSON Prompting, Chaining, and Streaming\n\n**CV Raman Global University, Bhubaneswar**  \n*AI Center of Excellence*\n\n</div>\n\n---\n\n### What You'll Learn\n\nIn this notebook, you will:\n\n1. **Use JSON structured outputs** to get predictable responses from LLMs\n2. **Chain multiple LLM calls** to build complex workflows\n3. **Implement streaming responses** for better user experience\n4. **Build a company brochure generator** as a practical project\n\n**Duration:** ~2 hours\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q openai requests beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from getpass import getpass\n",
    "from openai import OpenAI\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from IPython.display import Markdown, display, update_display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Configure Gemini\napi_key = getpass(\"Enter your Google Gemini API Key: \")\n\nif api_key and api_key.strip():\n    print(\"✅ API key accepted. You're ready to go!\")\nelse:\n    print(\"⚠️ No API key entered. Please re-run this cell.\")\n\nGEMINI_BASE_URL = \"https://generativelanguage.googleapis.com/v1beta/openai/\"\nclient = OpenAI(base_url=GEMINI_BASE_URL, api_key=api_key)\nMODEL = \"gemini-2.0-flash\""
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## 2. Web Scraping Utilities\n\nWe'll reuse the two web scraping functions from notebook 01:\n\n- **`fetch_website_contents(url)`** — fetches a webpage's HTML, strips out scripts/styles/images, and returns the clean text content (truncated to a max length).\n- **`fetch_website_links(url)`** — fetches a webpage and returns a list of all hyperlink URLs found on it.\n\nThese are the building blocks we'll feed into our LLM calls later."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36\"\n",
    "}\n",
    "\n",
    "def fetch_website_contents(url, max_chars=2000):\n",
    "    \"\"\"Fetch and return the text content of a website.\"\"\"\n",
    "    response = requests.get(url, headers=HEADERS)\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    \n",
    "    title = soup.title.string if soup.title else \"No title found\"\n",
    "    \n",
    "    if soup.body:\n",
    "        for irrelevant in soup.body([\"script\", \"style\", \"img\", \"input\"]):\n",
    "            irrelevant.decompose()\n",
    "        text = soup.body.get_text(separator=\"\\n\", strip=True)\n",
    "    else:\n",
    "        text = \"\"\n",
    "    \n",
    "    return (title + \"\\n\\n\" + text)[:max_chars]\n",
    "\n",
    "\n",
    "def fetch_website_links(url):\n",
    "    \"\"\"Return all links found on a webpage.\"\"\"\n",
    "    response = requests.get(url, headers=HEADERS)\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    links = [link.get(\"href\") for link in soup.find_all(\"a\")]\n",
    "    return [link for link in links if link]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. JSON Structured Outputs\n",
    "\n",
    "When you need predictable, parseable responses from an LLM, use **JSON mode**.\n",
    "\n",
    "This is essential for:\n",
    "- Building pipelines where output feeds into code\n",
    "- Extracting structured data from text\n",
    "- Creating reliable automation\n",
    "\n",
    "### One-Shot Prompting\n",
    "\n",
    "We provide an example in the prompt to show the expected format:"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "### Try It: Simple JSON Mode Example\n\nLet's start with a simple example to see JSON mode in action. We'll ask the model to return structured data about a topic:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Simple JSON example — ask the model to return structured data\nresponse = client.chat.completions.create(\n    model=MODEL,\n    messages=[\n        {\"role\": \"system\", \"content\": \"Extract information and respond in JSON with keys: topic, summary, difficulty\"},\n        {\"role\": \"user\", \"content\": \"Explain what web scraping is\"}\n    ],\n    response_format={\"type\": \"json_object\"}\n)\n\nresult = json.loads(response.choices[0].message.content)\nprint(json.dumps(result, indent=2))",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Applying JSON Mode: Link Selection for a Brochure\n\nNow let's use JSON mode for a real task. We'll ask the LLM to analyze a list of links from a website and pick the ones most relevant for a company brochure.\n\nFirst, we define a **system prompt** that includes a JSON example (one-shot prompting) so the model knows exactly what format to return:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System prompt with JSON example (one-shot prompting)\n",
    "LINK_SYSTEM_PROMPT = \"\"\"\n",
    "You are provided with a list of links found on a webpage.\n",
    "Decide which links would be most relevant for a company brochure,\n",
    "such as About page, Company page, or Careers/Jobs pages.\n",
    "Respond in JSON as in this example:\n",
    "\n",
    "{\n",
    "    \"links\": [\n",
    "        {\"type\": \"about page\", \"url\": \"https://full.url/goes/here/about\"},\n",
    "        {\"type\": \"careers page\", \"url\": \"https://another.full.url/careers\"}\n",
    "    ]\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "source": "Now let's build the user prompt that sends the list of links to the model. This function fetches all links from a webpage and formats them into a prompt:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_links_prompt(url):\n",
    "    \"\"\"Create the user prompt for link selection.\"\"\"\n",
    "    links = fetch_website_links(url)\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "Here is the list of links on the website {url} -\n",
    "Please decide which are relevant for a company brochure.\n",
    "Respond with the full https URL in JSON format.\n",
    "Do not include Terms of Service, Privacy, or email links.\n",
    "\n",
    "Links:\n",
    "\"\"\"\n",
    "    prompt += \"\\n\".join(links[:50])  # Limit to first 50 links\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "Finally, we combine the system prompt and user prompt into an API call, and use `response_format` to force the model to return valid JSON:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_relevant_links(url):\n",
    "    \"\"\"Use LLM to select relevant links from a website.\"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=MODEL,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": LINK_SYSTEM_PROMPT},\n",
    "            {\"role\": \"user\", \"content\": create_links_prompt(url)}\n",
    "        ],\n",
    "        response_format={\"type\": \"json_object\"}  # Force JSON output\n",
    "    )\n",
    "    \n",
    "    result = response.choices[0].message.content\n",
    "    return json.loads(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test link selection\n",
    "links = select_relevant_links(\"https://anthropic.com\")\n",
    "print(json.dumps(links, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Chaining LLM Calls\n",
    "\n",
    "**Chaining** means using the output of one LLM call as input to another.\n",
    "\n",
    "This is an early example of **Agentic AI** patterns.\n",
    "\n",
    "### Our Pipeline:\n",
    "1. **Step 1**: Extract relevant links from website (using JSON output)\n",
    "2. **Step 2**: Fetch content from those links\n",
    "3. **Step 3**: Generate brochure from aggregated content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def fetch_page_and_relevant_links(url):\n    \"\"\"Fetch main page and content from relevant links.\"\"\"\n    # Step 1: Get the main page content\n    contents = fetch_website_contents(url)\n    \n    # Step 2: Use LLM to pick relevant links (this is the \"chain\" — LLM output drives next steps)\n    relevant_links = select_relevant_links(url)\n    \n    # Step 3: Combine everything into one string\n    result = f\"## Landing Page:\\n\\n{contents}\\n\\n## Relevant Links:\\n\"\n    \n    for link in relevant_links.get('links', [])[:3]:  # Limit to 3 links\n        result += f\"\\n\\n### {link['type']}\\n\"\n        try:\n            result += fetch_website_contents(link[\"url\"])\n        except:\n            result += \"(Could not fetch content)\"\n    \n    return result"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test content aggregation\n",
    "content = fetch_page_and_relevant_links(\"https://anthropic.com\")\n",
    "print(content[:1000] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Building the Brochure Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BROCHURE_SYSTEM_PROMPT = \"\"\"\n",
    "You are an assistant that analyzes company website content\n",
    "and creates a professional brochure for prospective customers, investors, and recruits.\n",
    "Respond in markdown without code blocks.\n",
    "Include details of company culture, products/services, and careers if available.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "source": "**Note:** `create_brochure_prompt` calls `fetch_page_and_relevant_links` internally, which makes an LLM call to select relevant links. So generating a brochure involves **two LLM calls** — that's the chain!",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_brochure_prompt(company_name, url):\n",
    "    \"\"\"Create the prompt for brochure generation.\"\"\"\n",
    "    prompt = f\"\"\"\n",
    "You are looking at a company called: {company_name}\n",
    "Here are the contents of its landing page and other relevant pages.\n",
    "Use this information to build a short brochure in markdown.\n",
    "\n",
    "\"\"\"\n",
    "    prompt += fetch_page_and_relevant_links(url)\n",
    "    return prompt[:5000]  # Truncate to fit context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_brochure(company_name, url):\n",
    "    \"\"\"Generate a company brochure.\"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=MODEL,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": BROCHURE_SYSTEM_PROMPT},\n",
    "            {\"role\": \"user\", \"content\": create_brochure_prompt(company_name, url)}\n",
    "        ]\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a brochure\n",
    "brochure = create_brochure(\"Anthropic\", \"https://anthropic.com\")\n",
    "display(Markdown(brochure))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Streaming Responses\n",
    "\n",
    "**Streaming** shows the response as it's generated, providing a better user experience.\n",
    "\n",
    "Instead of waiting for the complete response, you see text appear in real-time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def stream_brochure(company_name, url, system_prompt=BROCHURE_SYSTEM_PROMPT):\n    \"\"\"Generate a brochure with streaming output.\"\"\"\n    stream = client.chat.completions.create(\n        model=MODEL,\n        messages=[\n            {\"role\": \"system\", \"content\": system_prompt},\n            {\"role\": \"user\", \"content\": create_brochure_prompt(company_name, url)}\n        ],\n        stream=True  # Enable streaming\n    )\n    \n    response = \"\"\n    display_handle = display(Markdown(\"\"), display_id=True)\n    \n    for chunk in stream:\n        content = chunk.choices[0].delta.content or ''\n        response += content\n        update_display(Markdown(response), display_id=display_handle.display_id)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test streaming\n",
    "stream_brochure(\"Anthropic\", \"https://anthropic.com\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Changing Tone with System Prompts\n",
    "\n",
    "You can easily change the output style by modifying the system prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Just define a different system prompt — reuse the same function!\nHUMOROUS_SYSTEM_PROMPT = \"\"\"\nYou are an assistant that creates witty, entertaining brochures about companies.\nUse humor and clever observations while still being informative.\nRespond in markdown without code blocks.\n\"\"\""
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Try the humorous version — same function, different prompt\n# stream_brochure(\"Anthropic\", \"https://anthropic.com\", system_prompt=HUMOROUS_SYSTEM_PROMPT)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Exercise: Build a Product Description Generator\n",
    "\n",
    "Apply what you've learned to create a different application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Exercise: Create a product description generator\n\n# Step 1: Define your system prompt\n# What should the model do? What tone and format should the output have?\nPRODUCT_SYSTEM_PROMPT = \"\"  # Fill this in\n\n# Step 2: Create a function to fetch and format product page content\n# Hint: Use fetch_website_contents() to get the page text\n# def create_product_prompt(product_url):\n#     content = fetch_website_contents(product_url)\n#     return f\"Create marketing copy for this product:\\n\\n{content}\"\n\n# Step 3: Create the generator function with streaming\n# Hint: You can reuse the streaming pattern from stream_brochure\n# def generate_product_description(product_url):\n#     stream = client.chat.completions.create(\n#         model=MODEL,\n#         messages=[\n#             {\"role\": \"system\", \"content\": PRODUCT_SYSTEM_PROMPT},\n#             {\"role\": \"user\", \"content\": create_product_prompt(product_url)}\n#         ],\n#         stream=True\n#     )\n#     response = \"\"\n#     display_handle = display(Markdown(\"\"), display_id=True)\n#     for chunk in stream:\n#         content = chunk.choices[0].delta.content or ''\n#         response += content\n#         update_display(Markdown(response), display_id=display_handle.display_id)\n\n# Step 4: Test it with a real product page\n# generate_product_description(\"https://example.com/product\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Key Takeaways\n\n1. **JSON mode** (`response_format={\"type\": \"json_object\"}`) ensures parseable outputs\n\n2. **One-shot prompting** - provide an example in the prompt for better formatting\n\n3. **Chaining LLM calls** creates powerful pipelines - output of one feeds into another\n\n4. **Streaming** (`stream=True`) provides better UX with real-time output\n\n5. **Tone control** - system prompts easily change the style of output\n\n### Pipeline Pattern\n\n```\nInput → LLM Call 1 (Extract/Analyze) → LLM Call 2 (Generate) → Output\n```\n\nThis is an early form of **Agentic AI** - we'll explore this more in later units!\n\n---\n\n## Additional Resources\n\n- [Gemini API - JSON Mode](https://ai.google.dev/gemini-api/docs/openai)\n- [Gemini API - Streaming](https://ai.google.dev/gemini-api/docs/openai)\n\n---\n\n**Course Information:**\n- **Institution:** CV Raman Global University, Bhubaneswar\n- **Program:** AI Center of Excellence\n- **Course:** AI Systems Engineering 1\n- **Developed by:** [Poorit Technologies](https://poorit.in) - *Transform Graduates into Industry-Ready Professionals*\n\n---"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}