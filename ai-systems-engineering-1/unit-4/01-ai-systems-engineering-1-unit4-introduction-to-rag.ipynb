{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "<img src=\"https://poorit.in/image.png\" alt=\"Poorit\" width=\"40\" style=\"vertical-align: middle;\"> <b>AI SYSTEMS ENGINEERING 1</b>\n",
    "\n",
    "## Unit 4: Introduction to RAG\n",
    "\n",
    "**CV Raman Global University, Bhubaneswar**  \n",
    "*AI Center of Excellence*\n",
    "\n",
    "---\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Poorit-Technologies/cvraman-coe/blob/main/courses-contents/ai-systems-engineering-1/unit-4/01-ai-systems-engineering-1-unit4-introduction-to-rag.ipynb)\n",
    "\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "### What You'll Learn\n",
    "\n",
    "In this notebook, you will:\n",
    "\n",
    "1. **Understand why RAG exists** - see LLMs fail without context\n",
    "2. **Learn what RAG is** - Retrieval Augmented Generation explained\n",
    "3. **Understand LLM parameters** - temperature, max_tokens, and more\n",
    "4. **Build a simple knowledge base** from documents\n",
    "5. **Implement basic retrieval** using keyword matching\n",
    "6. **Create a Q&A chatbot** with context injection\n",
    "7. **See the RAG pipeline in action** - step-by-step visibility\n",
    "\n",
    "**Duration:** ~2 hours\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Install required packages\n!pip install -q litellm gradio"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\nfrom getpass import getpass\nfrom litellm import completion\nimport gradio as gr"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Configure API\napi_key = getpass(\"Enter your OpenAI API Key: \")\nos.environ['OPENAI_API_KEY'] = api_key\n\nMODEL = \"gpt-4o-mini\""
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. The Problem — LLMs Don't Know Everything\n",
    "\n",
    "Before we build a RAG system, let's understand **why** we need one.\n",
    "\n",
    "LLMs are trained on public internet data up to a cutoff date. They **don't know** about:\n",
    "\n",
    "| What LLMs Don't Know | Example |\n",
    "|---|---|\n",
    "| **Your company's internal data** | Employee details, policies, pricing |\n",
    "| **Recent events** | News after the training cutoff |\n",
    "| **Private documents** | Internal reports, meeting notes |\n",
    "| **Domain-specific knowledge** | Niche industry data, local context |\n",
    "\n",
    "Let's see this in action. We'll ask GPT about our fictional company **TechSolutions India** — which doesn't exist in its training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Ask about something the LLM doesn't know — no context provided\nresponse = completion(\n    model=MODEL,\n    messages=[\n        {\"role\": \"user\", \"content\": \"Who is the CEO of TechSolutions India and what is the company's annual revenue?\"}\n    ],\n    temperature=0\n)\n\nprint(\"Question: Who is the CEO of TechSolutions India and what is the company's annual revenue?\")\nprint(f\"\\nLLM Response (no context):\\n{response.choices[0].message.content}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation:** The LLM either made something up (hallucination) or admitted it doesn't know. Either way, it **can't answer accurately** because TechSolutions India doesn't exist in its training data.\n",
    "\n",
    "Now let's try the same question, but this time we'll **manually inject the relevant information** into the prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Same question, but now we provide context\ncontext = \"\"\"\nTechSolutions India is a software development company founded in 2018.\nHeadquarters: Bhubaneswar, Odisha\nEmployees: 250+\nAnnual Revenue: ₹50 crores (2024)\nCEO: Priya Sharma\n\"\"\"\n\nresponse = completion(\n    model=MODEL,\n    messages=[\n        {\"role\": \"system\", \"content\": f\"Use the following context to answer the question:\\n\\n{context}\"},\n        {\"role\": \"user\", \"content\": \"Who is the CEO of TechSolutions India and what is the company's annual revenue?\"}\n    ],\n    temperature=0\n)\n\nprint(\"Question: Who is the CEO of TechSolutions India and what is the company's annual revenue?\")\nprint(f\"\\nLLM Response (with context):\\n{response.choices[0].message.content}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Key Insight\n",
    "\n",
    "When we **gave the LLM the right information**, it answered perfectly. That's the core idea behind RAG:\n",
    "\n",
    "> **Don't expect the LLM to know everything. Find the right information and give it to the LLM. That's RAG.**\n",
    "\n",
    "The question is: how do we **automatically** find the right information for any question? That's what we'll build in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. What is RAG?\n",
    "\n",
    "**Retrieval Augmented Generation (RAG)** is a technique that gives LLMs access to external knowledge by finding relevant information and injecting it into the prompt.\n",
    "\n",
    "### The Open-Book Exam Analogy\n",
    "\n",
    "Think of it like the difference between a closed-book and open-book exam:\n",
    "\n",
    "| | Closed-Book (LLM Alone) | Open-Book (LLM + RAG) |\n",
    "|---|---|---|\n",
    "| **Knowledge source** | Only what's memorized (training data) | Can look up reference materials |\n",
    "| **Accuracy** | May guess or hallucinate | Answers grounded in real documents |\n",
    "| **Current info** | Frozen at training cutoff | Can access up-to-date information |\n",
    "| **Domain knowledge** | General knowledge only | Can use specialized documents |\n",
    "\n",
    "### The New Employee Analogy\n",
    "\n",
    "Imagine a smart new employee on their first day:\n",
    "\n",
    "1. Someone asks them a question about company policy\n",
    "2. They **search** the company wiki for relevant pages\n",
    "3. They **read** the relevant section\n",
    "4. They **answer** using what they found\n",
    "\n",
    "That's exactly what RAG does — but with an LLM instead of an employee!\n",
    "\n",
    "### The RAG Pipeline\n",
    "\n",
    "```\n",
    "┌──────────────┐     ┌──────────────┐     ┌──────────────────┐     ┌──────────────┐\n",
    "│   QUESTION   │────>│  RETRIEVAL   │────>│   AUGMENTATION   │────>│  GENERATION  │\n",
    "│              │     │              │     │                  │     │              │\n",
    "│ \"Who is the  │     │ Search the   │     │ Add retrieved    │     │ LLM generates│\n",
    "│  CEO?\"       │     │ knowledge    │     │ docs to the      │     │ answer using │\n",
    "│              │     │ base         │     │ prompt as context│     │ the context  │\n",
    "└──────────────┘     └──────────────┘     └──────────────────┘     └──────────────┘\n",
    "```\n",
    "\n",
    "### Why RAG?\n",
    "\n",
    "| Problem | RAG Solution |\n",
    "|---------|-------------|\n",
    "| LLM knowledge cutoff | Provide up-to-date information |\n",
    "| Hallucinations | Ground answers in real documents |\n",
    "| Domain expertise | Add company-specific knowledge |\n",
    "| Cost | Cheaper than fine-tuning |\n",
    "| Transparency | Can cite sources for answers |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Understanding LLM Parameters\n",
    "\n",
    "Throughout this course, we've been passing parameters like `temperature=0` to our LLM calls. Let's understand what these mean.\n",
    "\n",
    "### Temperature — Controlling Randomness\n",
    "\n",
    "Temperature controls how \"creative\" vs \"deterministic\" the LLM's responses are:\n",
    "\n",
    "```\n",
    "Temperature Scale:\n",
    "\n",
    " 0.0          0.7          1.0          1.5          2.0\n",
    "  |------------|------------|------------|------------|\n",
    "  Deterministic  Balanced     Creative     Wild      Chaotic\n",
    "  (factual)    (default)   (storytelling)           (nonsensical)\n",
    "```\n",
    "\n",
    "| Temperature | Behavior | Best For |\n",
    "|---|---|---|\n",
    "| **0.0** | Always picks the most likely word. Same input = same output. | Factual Q&A, RAG, data extraction |\n",
    "| **0.3 - 0.5** | Mostly deterministic with slight variation | Summarization, translation |\n",
    "| **0.7** | Balanced creativity (OpenAI default) | General conversation, writing |\n",
    "| **1.0** | More diverse and creative outputs | Brainstorming, creative writing |\n",
    "| **> 1.0** | Increasingly random and unpredictable | Rarely useful in practice |\n",
    "\n",
    "Let's see the difference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Compare temperature effects — ask the same question multiple times\nquestion = \"Suggest a one-sentence tagline for a tech company in Bhubaneswar.\"\n\nfor temp in [0, 0.7, 1.0]:\n    print(f\"{'='*60}\")\n    print(f\"Temperature = {temp}\")\n    print(f\"{'='*60}\")\n    \n    for attempt in range(1, 3):\n        response = completion(\n            model=MODEL,\n            messages=[\n                {\"role\": \"system\", \"content\": \"You are a marketing assistant. Reply with only the tagline, nothing else.\"},\n                {\"role\": \"user\", \"content\": question}\n            ],\n            temperature=temp\n        )\n        print(f\"  Attempt {attempt}: {response.choices[0].message.content}\")\n    \n    print()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notice:**\n",
    "- At **temperature=0**, both attempts produce the **same** tagline\n",
    "- At **temperature=0.7**, you get **slight variations**\n",
    "- At **temperature=1.0**, the taglines are **noticeably different** each time\n",
    "\n",
    "### Why RAG Uses Temperature = 0\n",
    "\n",
    "For RAG systems, we want **consistent, factual answers** — not creative ones. When we have the right context, we want the LLM to faithfully report what's in the documents, not improvise. That's why we set `temperature=0`.\n",
    "\n",
    "### Other Useful Parameters\n",
    "\n",
    "| Parameter | What It Does | Common Values |\n",
    "|---|---|---|\n",
    "| **`max_tokens`** | Maximum length of the response | 100 - 4000 |\n",
    "| **`top_p`** | Alternative to temperature (nucleus sampling) | 0.0 - 1.0 |\n",
    "| **`stream`** | Return response word-by-word | `True` / `False` |\n",
    "\n",
    "> **Tip:** Don't set both `temperature` and `top_p` at the same time — use one or the other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Create a Sample Knowledge Base\n",
    "\n",
    "Let's create a knowledge base for a fictional company."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample knowledge base for TechSolutions India\n",
    "knowledge_base = {\n",
    "    \"company\": \"\"\"\n",
    "TechSolutions India is a software development company founded in 2018.\n",
    "Headquarters: Bhubaneswar, Odisha\n",
    "Employees: 250+\n",
    "Specialization: AI/ML solutions, Cloud services, Mobile apps\n",
    "Annual Revenue: ₹50 crores (2024)\n",
    "CEO: Priya Sharma\n",
    "\"\"\",\n",
    "    \n",
    "    \"priya\": \"\"\"\n",
    "Priya Sharma - CEO and Co-founder\n",
    "Education: IIT Delhi (B.Tech), Stanford (MBA)\n",
    "Experience: 15 years in tech industry\n",
    "Previous: Senior Director at Infosys\n",
    "Awards: Forbes 30 Under 30 (2015), Women in Tech Leader (2022)\n",
    "Email: priya@techsolutions.in\n",
    "\"\"\",\n",
    "    \n",
    "    \"rahul\": \"\"\"\n",
    "Rahul Verma - CTO and Co-founder\n",
    "Education: BITS Pilani (B.Tech), IIM Bangalore (MBA)\n",
    "Experience: 12 years in software development\n",
    "Previous: Tech Lead at Google India\n",
    "Expertise: Machine Learning, Cloud Architecture\n",
    "Email: rahul@techsolutions.in\n",
    "\"\"\",\n",
    "    \n",
    "    \"products\": \"\"\"\n",
    "TechSolutions Products:\n",
    "\n",
    "1. CloudAssist Pro - Enterprise cloud management platform\n",
    "   - Price: ₹50,000/month\n",
    "   - Features: Auto-scaling, monitoring, cost optimization\n",
    "\n",
    "2. SmartHR - AI-powered HR management system\n",
    "   - Price: ₹25,000/month\n",
    "   - Features: Recruitment, payroll, performance tracking\n",
    "\n",
    "3. DataViz Analytics - Business intelligence dashboard\n",
    "   - Price: ₹15,000/month\n",
    "   - Features: Real-time analytics, custom reports\n",
    "\"\"\",\n",
    "    \n",
    "    \"policies\": \"\"\"\n",
    "Company Policies:\n",
    "\n",
    "Work Hours: 9 AM - 6 PM, Monday to Friday\n",
    "Leave Policy: 24 paid leaves + 10 sick leaves per year\n",
    "Remote Work: Hybrid model - 3 days office, 2 days remote\n",
    "Probation: 6 months for all new employees\n",
    "Notice Period: 2 months for permanent employees\n",
    "\"\"\",\n",
    "    \n",
    "    \"benefits\": \"\"\"\n",
    "Employee Benefits:\n",
    "\n",
    "- Health Insurance: ₹5 lakh coverage for employee + family\n",
    "- Performance Bonus: Up to 20% of annual salary\n",
    "- Learning Budget: ₹50,000/year for courses and certifications\n",
    "- Gym Membership: Fully covered\n",
    "- Team Outings: Quarterly team events\n",
    "\"\"\"\n",
    "}\n",
    "\n",
    "print(f\"Knowledge base has {len(knowledge_base)} documents\")\n",
    "print(f\"Topics: {list(knowledge_base.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Simple Keyword-Based Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relevant_context(message):\n",
    "    \"\"\"Find relevant documents based on keyword matching.\"\"\"\n",
    "    # Extract words from message\n",
    "    text = ''.join(ch for ch in message if ch.isalpha() or ch.isspace())\n",
    "    words = text.lower().split()\n",
    "    \n",
    "    # Find matching documents\n",
    "    relevant = []\n",
    "    for word in words:\n",
    "        if word in knowledge_base:\n",
    "            relevant.append(knowledge_base[word])\n",
    "    \n",
    "    return relevant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test retrieval\n",
    "question = \"Who is Priya?\"\n",
    "context = get_relevant_context(question)\n",
    "\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Found {len(context)} relevant documents\")\n",
    "if context:\n",
    "    print(f\"\\nContext:\\n{context[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with multiple keywords\n",
    "question = \"Tell me about the company and its products\"\n",
    "context = get_relevant_context(question)\n",
    "\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Found {len(context)} relevant documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Build the RAG System\n",
    "\n",
    "Now we combine retrieval with LLM generation. Note the use of `temperature=0` — as we learned in Section 4, this ensures factual, consistent answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"\n",
    "You are a helpful assistant for TechSolutions India.\n",
    "You answer questions about the company, its employees, products, and policies.\n",
    "Use the provided context to answer questions accurately.\n",
    "If you don't know the answer or it's not in the context, say so.\n",
    "Keep answers concise and professional.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\"\"\"\n",
    "\n",
    "def format_context(relevant_docs):\n",
    "    \"\"\"Format retrieved documents as context.\"\"\"\n",
    "    if not relevant_docs:\n",
    "        return \"No specific context available for this question.\"\n",
    "    return \"\\n\\n---\\n\\n\".join(relevant_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def answer_question(question, history=[]):\n    \"\"\"Answer a question using RAG.\"\"\"\n    # Step 1: Retrieve relevant context\n    relevant_docs = get_relevant_context(question)\n    context = format_context(relevant_docs)\n    \n    # Step 2: Create prompt with context\n    system_message = SYSTEM_PROMPT.format(context=context)\n    \n    # Step 3: Generate answer\n    messages = [\n        {\"role\": \"system\", \"content\": system_message}\n    ] + history + [\n        {\"role\": \"user\", \"content\": question}\n    ]\n    \n    response = completion(\n        model=MODEL,\n        messages=messages,\n        temperature=0\n    )\n    \n    return response.choices[0].message.content"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the RAG system\n",
    "questions = [\n",
    "    \"Who is the CEO of the company?\",\n",
    "    \"What products does TechSolutions offer?\",\n",
    "    \"What is the leave policy?\",\n",
    "    \"Tell me about Rahul\"\n",
    "]\n",
    "\n",
    "for q in questions:\n",
    "    print(f\"Q: {q}\")\n",
    "    answer = answer_question(q)\n",
    "    print(f\"A: {answer}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Build a Chat Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(message, history):\n",
    "    \"\"\"Chat function for Gradio.\"\"\"\n",
    "    return answer_question(message, history)\n",
    "\n",
    "# Launch chat interface\n",
    "demo = gr.ChatInterface(\n",
    "    chat,\n",
    "    title=\"TechSolutions Assistant\",\n",
    "    description=\"Ask questions about TechSolutions India - company, employees, products, and policies.\",\n",
    "    examples=[\n",
    "        \"Who founded the company?\",\n",
    "        \"What are the employee benefits?\",\n",
    "        \"How much does CloudAssist Pro cost?\"\n",
    "    ],\n",
    "    type=\"messages\"\n",
    ")\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. See the RAG Pipeline in Action\n",
    "\n",
    "Our `answer_question()` function works, but it hides what's happening inside. Let's create a **verbose version** that shows each step of the RAG pipeline as it runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def answer_question_verbose(question):\n    \"\"\"Answer a question using RAG, showing each pipeline stage.\"\"\"\n    \n    print(f\"{'='*60}\")\n    print(\"RAG PIPELINE — Step by Step\")\n    print(f\"{'='*60}\")\n    \n    # Step 1: Question\n    print(f\"\\n[STEP 1] USER QUESTION\")\n    print(f'   \"{question}\"')\n    \n    # Step 2: Retrieval\n    print(f\"\\n[STEP 2] RETRIEVAL\")\n    relevant_docs = get_relevant_context(question)\n    print(f\"   Found {len(relevant_docs)} relevant document(s)\")\n    if relevant_docs:\n        for i, doc in enumerate(relevant_docs, 1):\n            preview = doc.strip()[:100].replace('\\n', ' ')\n            print(f\"   Doc {i}: {preview}...\")\n    else:\n        print(\"   No matching documents found.\")\n    \n    # Step 3: Augmentation\n    print(f\"\\n[STEP 3] AUGMENTATION\")\n    context = format_context(relevant_docs)\n    system_message = SYSTEM_PROMPT.format(context=context)\n    messages = [\n        {\"role\": \"system\", \"content\": system_message},\n        {\"role\": \"user\", \"content\": question}\n    ]\n    print(f\"   Context length: {len(context)} characters\")\n    print(f\"   Total prompt length: {sum(len(m['content']) for m in messages)} characters\")\n    \n    # Step 4: Generation\n    print(f\"\\n[STEP 4] GENERATION\")\n    response = completion(\n        model=MODEL,\n        messages=messages,\n        temperature=0\n    )\n    answer = response.choices[0].message.content\n    print(f\"   {answer}\")\n    \n    print(f\"\\n{'='*60}\")\n    return answer\n\n\n# Test with example questions\ntest_questions = [\n    \"What are the employee benefits?\",\n    \"Who is Rahul?\",\n    \"What is the dress code?\"  # Not in knowledge base\n]\n\nfor q in test_questions:\n    answer_question_verbose(q)\n    print()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 10. Limitations of Simple RAG\n",
    "\n",
    "Our simple keyword-based retrieval has limitations:\n",
    "\n",
    "| Limitation | Example |\n",
    "|-----------|--------|\n",
    "| **Exact match only** | \"CEO\" won't match \"priya\" document |\n",
    "| **No semantic understanding** | \"founder\" won't find CEO info |\n",
    "| **No ranking** | Can't prioritize more relevant docs |\n",
    "| **Limited scalability** | Doesn't work for large knowledge bases |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate limitation\n",
    "question = \"Who founded the company?\"  # Uses 'founded' not 'priya'\n",
    "context = get_relevant_context(question)\n",
    "\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Found {len(context)} documents\")  # Likely 0 or just 'company'\n",
    "\n",
    "# But the answer exists in the 'company' and 'priya' documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 11. Exercise: Expand the Knowledge Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise: Add more documents to the knowledge base\n",
    "# 1. Add a document about \"careers\" or \"jobs\"\n",
    "# 2. Add a document about \"clients\" or \"customers\"\n",
    "# 3. Test with new questions\n",
    "\n",
    "# Your implementation here\n",
    "# knowledge_base[\"careers\"] = \"\"\"...\"\"\"\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## 12. Key Takeaways\n\n1. **LLMs don't know everything** — they can hallucinate or refuse when asked about data outside their training\n\n2. **RAG = Retrieval + Augmentation + Generation** — find relevant documents, inject them as context, then generate an answer\n\n3. **Temperature controls randomness** — RAG uses `temperature=0` for consistent, factual responses\n\n4. **Context is key** — the quality of retrieved documents directly affects answer quality\n\n5. **Simple retrieval works** — keyword matching is a good starting point, but has limitations\n\n6. **System prompts matter** — they instruct the LLM how to use the provided context\n\n### RAG Stage to Code Mapping\n\n| RAG Stage | Our Function | What It Does |\n|---|---|---|\n| **Retrieval** | `get_relevant_context()` | Searches knowledge base by keyword |\n| **Augmentation** | `format_context()` + `SYSTEM_PROMPT` | Formats docs and injects into prompt |\n| **Generation** | `completion()` | LLM generates answer from context |\n\n### What's Next?\n\nIn the next notebook, we'll improve our retrieval system using:\n- **Vector embeddings** — represent text as numbers to capture meaning\n- **Semantic search** — find documents by meaning, not just keywords\n- **ChromaDB** — a vector database purpose-built for RAG\n\n---\n\n## Additional Resources\n\n- [RAG Paper](https://arxiv.org/abs/2005.11401) — the original research paper\n- [LangChain RAG Tutorial](https://python.langchain.com/docs/tutorials/rag/)\n- [LiteLLM Documentation](https://docs.litellm.ai/) — unified API for 100+ LLM providers\n- [OpenAI API Parameters](https://platform.openai.com/docs/api-reference/chat/create) — full list of parameters\n- [IBM: What is RAG? (video)](https://www.ibm.com/think/videos/rag) — short visual explainer\n- [RAG Playground (interactive)](https://ragplay.vercel.app/) — see chunking, embeddings, and retrieval live\n- [RAG Pipeline Diagrams](https://www.designveloper.com/blog/rag-pipeline-diagram/) — step-by-step visual guide\n- [DeepLearning.AI RAG Course](https://learn.deeplearning.ai/courses/retrieval-augmented-generation/) — free short course\n\n---\n\n**Course Information:**\n- **Institution:** CV Raman Global University, Bhubaneswar\n- **Program:** AI Center of Excellence\n- **Course:** AI Systems Engineering 1\n- **Developed by:** [Poorit Technologies](https://poorit.in) - *Transform Graduates into Industry-Ready Professionals*\n\n---"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}