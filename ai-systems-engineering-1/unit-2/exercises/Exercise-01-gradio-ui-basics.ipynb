{
 "nbformat": 4,
 "nbformat_minor": 4,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "<img src=\"https://poorit.in/image.png\" alt=\"Poorit\" width=\"40\" style=\"vertical-align: middle;\"> <b>AI SYSTEMS ENGINEERING 1</b>\n",
    "\n",
    "## Unit 2 Exercises: Gradio UI Basics\n",
    "\n",
    "**CV Raman Global University, Bhubaneswar**  \n",
    "*AI Center of Excellence*\n",
    "\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "Complete the exercises below using the setup provided. Exercises include both **fill-in-the-blank** and **coding** tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Run the cells below to install packages, import libraries, and define helper functions."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Install required packages\n",
    "!pip install -q openai gradio requests beautifulsoup4"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "from openai import OpenAI\n",
    "import gradio as gr\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Configure API keys\n",
    "openai_api_key = getpass(\"Enter your OpenAI API Key: \")\n",
    "os.environ['OPENAI_API_KEY'] = openai_api_key\n",
    "\n",
    "# Optional: Google API key for Gemini\n",
    "google_api_key = getpass(\"Enter your Google API Key (or press Enter to skip): \")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Initialize clients\n",
    "openai_client = OpenAI(api_key=openai_api_key)\n",
    "\n",
    "GEMINI_URL = \"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    "gemini_client = OpenAI(api_key=google_api_key, base_url=GEMINI_URL) if google_api_key else None\n",
    "\n",
    "MODEL = \"gpt-4o-mini\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Helper function for web scraping\n",
    "\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\"\n",
    "}\n",
    "\n",
    "def fetch_website_contents(url, max_chars=2000):\n",
    "    \"\"\"Fetch and return the text content of a website.\"\"\"\n",
    "    try:\n",
    "        response = requests.get(url, headers=HEADERS, timeout=10)\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "        title = soup.title.string if soup.title else \"No title found\"\n",
    "\n",
    "        if soup.body:\n",
    "            for irrelevant in soup.body([\"script\", \"style\", \"img\", \"input\"]):\n",
    "                irrelevant.decompose()\n",
    "            text = soup.body.get_text(separator=\"\\n\", strip=True)\n",
    "        else:\n",
    "            text = \"\"\n",
    "\n",
    "        return (title + \"\\n\\n\" + text)[:max_chars]\n",
    "    except Exception as e:\n",
    "        return f\"Error fetching website: {str(e)}\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Q1: Basic Gradio + LLM Interface \u2014 Fill-in-the-Blank\n",
    "\n",
    "Create a Gradio interface that connects to GPT. Fill in the blanks to:\n",
    "1. Define a system message\n",
    "2. Build the `messages` list with the correct roles\n",
    "3. Configure `gr.Interface` with the correct parameters"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Define a system message that tells the model to respond in markdown\n",
    "system_message = \"___\"\n",
    "\n",
    "def message_gpt(prompt):\n",
    "    \"\"\"Send a message to GPT and return the response.\"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"___\", \"content\": system_message},\n",
    "        {\"role\": \"___\", \"content\": prompt}\n",
    "    ]\n",
    "    response = openai_client.chat.completions.create(model=MODEL, messages=___)\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Create the Gradio interface\n",
    "view = gr.Interface(\n",
    "    fn=___,\n",
    "    inputs=___,\n",
    "    outputs=gr.Markdown(label=\"Response:\"),\n",
    "    title=\"GPT Assistant\",\n",
    "    flagging_mode=\"never\"\n",
    ")\n",
    "view.launch()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Q2: Add Streaming \u2014 Fill-in-the-Blank\n",
    "\n",
    "Modify the LLM call to use **streaming** so the response appears word-by-word. Fill in the blanks to:\n",
    "1. Enable streaming in the API call\n",
    "2. Extract content from each chunk\n",
    "3. Use `yield` to stream the accumulated result"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def stream_gpt(prompt):\n",
    "    \"\"\"Stream a response from GPT.\"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_message},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "    stream = openai_client.chat.completions.create(\n",
    "        model=MODEL,\n",
    "        messages=messages,\n",
    "        ___=True  # Enable streaming\n",
    "    )\n",
    "\n",
    "    result = \"\"\n",
    "    for chunk in stream:\n",
    "        result += chunk.choices[0].___.content or \"\"  # Extract content from delta\n",
    "        ___ result  # Yield the accumulated result\n",
    "\n",
    "# Create the streaming interface\n",
    "view = gr.Interface(\n",
    "    fn=stream_gpt,\n",
    "    inputs=gr.Textbox(label=\"Your message:\", lines=5),\n",
    "    outputs=gr.Markdown(label=\"Response:\"),\n",
    "    title=\"GPT Assistant (Streaming)\",\n",
    "    flagging_mode=\"never\"\n",
    ")\n",
    "view.launch()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Written Response:** In 2\u20133 sentences, explain why streaming improves user experience compared to waiting for the full response.\n",
    "\n",
    "**Your Answer:** *(Write 2-3 sentences)*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Q3: Product Description Generator \u2014 Build from Requirements\n",
    "\n",
    "Build a complete Gradio application that generates marketing copy for products.\n",
    "\n",
    "### Requirements:\n",
    "1. **Define a system prompt** for a marketing copywriter that generates compelling product descriptions\n",
    "2. **Create a streaming function** `stream_product_description(product_name, url, model)` that:\n",
    "   - Fetches the product page content using `fetch_website_contents(url)`\n",
    "   - Sends the content to the selected model with your system prompt\n",
    "   - Streams the response back\n",
    "3. **Build a Gradio interface** with:\n",
    "   - A textbox for product name\n",
    "   - A textbox for product URL\n",
    "   - A dropdown for model selection (GPT / Gemini)\n",
    "   - Markdown output for the generated description\n",
    "   - At least 2 examples\n",
    "\n",
    "### Available helpers:\n",
    "- `fetch_website_contents(url)` \u2014 fetches and cleans website text\n",
    "- `openai_client` \u2014 OpenAI client (use with `MODEL`)\n",
    "- `gemini_client` \u2014 Gemini client (use with `\"gemini-1.5-flash\"`)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# YOUR CODE BELOW\n",
    "\n",
    "# Step 1: Define a system prompt for marketing copy generation\n",
    "product_system = \"\"\"\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Step 2: Create a streaming function\n",
    "def stream_product_description(product_name, url, model):\n",
    "    \"\"\"Generate a product description with streaming.\"\"\"\n",
    "    # 2a. Fetch the website content\n",
    "\n",
    "    # 2b. Build the prompt including product_name and website content\n",
    "\n",
    "    # 2c. Build the messages list with your system prompt\n",
    "\n",
    "    # 2d. Choose the right client and model_name based on `model` parameter\n",
    "\n",
    "    # 2e. Call the API with stream=True and yield results\n",
    "    pass\n",
    "\n",
    "\n",
    "# Step 3: Build the Gradio interface\n",
    "# 3a. Create input components (Textbox for name, Textbox for URL, Dropdown for model)\n",
    "# 3b. Create output component (Markdown)\n",
    "# 3c. Build gr.Interface with fn, inputs, outputs, title, examples\n",
    "# 3d. Launch the interface\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Written Response:** In 2\u20133 sentences, explain how the system prompt you chose shapes the style and tone of the generated product descriptions.\n",
    "\n",
    "**Your Answer:** *(Write 2-3 sentences)*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Course Information:**\n",
    "- **Institution:** CV Raman Global University, Bhubaneswar\n",
    "- **Program:** AI Center of Excellence\n",
    "- **Course:** AI Systems Engineering 1\n",
    "- **Developed by:** [Poorit Technologies](https://poorit.in) - *Transform Graduates into Industry-Ready Professionals*\n",
    "\n",
    "---"
   ]
  }
 ]
}