{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": "<div align=\"center\">\n<img src=\"https://poorit.in/image.png\" alt=\"Poorit\" width=\"40\" style=\"vertical-align: middle;\"> <b>AI SYSTEMS ENGINEERING 1</b>\n\n## Unit 2: Beyond Text - Images, Audio, and Multi-Model Access\n\n**CV Raman Global University, Bhubaneswar**  \n*AI Center of Excellence*\n\n</div>\n\n---\n\n### What You'll Learn\n\nIn this notebook, you will:\n\n1. **Use LiteLLM** to access multiple LLM providers with one interface\n2. **Generate images** from text prompts using gpt-image-1-mini\n3. **Create audio with text-to-speech** using OpenAI's TTS\n4. **Practice** with hands-on exercises\n\n**Duration:** ~45 minutes\n\n---"
  },
  {
   "cell_type": "markdown",
   "id": "setup-header",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "install",
   "metadata": {},
   "outputs": [],
   "source": "# Install required packages\n!pip install -q litellm pillow"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": "import os\nimport base64\nfrom io import BytesIO\nfrom getpass import getpass\nfrom litellm import completion, image_generation, speech\nfrom PIL import Image\nfrom IPython.display import Audio, display"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "api-key",
   "metadata": {},
   "outputs": [],
   "source": "# Configure API key\nos.environ['OPENAI_API_KEY'] = getpass(\"Enter your OpenAI API Key: \")"
  },
  {
   "cell_type": "markdown",
   "id": "litellm-header",
   "metadata": {},
   "source": "---\n\n## 2. One Interface for Many Models -- LiteLLM\n\nSo far we've used the OpenAI Python library directly. But what if you want to switch to Google Gemini, Anthropic Claude, or a local model?\n\n**LiteLLM** gives you a single `completion()` function that works with 100+ providers. You just change the model name -- the rest of your code stays the same.\n\n| Provider | Model Name in LiteLLM | API Key Env Variable |\n|----------|----------------------|---------------------|\n| OpenAI | `openai/gpt-4o-mini` | `OPENAI_API_KEY` |\n| Google | `gemini/gemini-2.0-flash` | `GOOGLE_API_KEY` |\n| Anthropic | `anthropic/claude-sonnet-4-5-20250929` | `ANTHROPIC_API_KEY` |\n| Local (Ollama) | `ollama/llama3.2` | -- |\n\nLet's try it with OpenAI first, then with Gemini:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "litellm-demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LiteLLM uses the same messages format you already know\n",
    "response = completion(\n",
    "    model=\"openai/gpt-4o-mini\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Tell me a fun fact about India in one sentence.\"}]\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)\n",
    "print(f\"\\nTokens used: {response.usage.total_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "id": "ezeat04rq9a",
   "source": "# Set up Google API key for Gemini\nos.environ['GOOGLE_API_KEY'] = getpass(\"Enter your Google API Key: \")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "ph5xopox3ii",
   "source": "# Same code pattern, different provider — just change the model string\nresponse = completion(\n    model=\"gemini/gemini-2.0-flash\",\n    messages=[{\"role\": \"user\", \"content\": \"Tell me a fun fact about India in one sentence.\"}]\n)\n\nprint(response.choices[0].message.content)\nprint(f\"\\nTokens used: {response.usage.total_tokens}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "litellm-note",
   "metadata": {},
   "source": "> **Key idea:** You just called two different providers (OpenAI and Google Gemini) with the exact same code pattern. The only change was the `model` string. That's the power of LiteLLM — one interface for 100+ models. And it's not just chat — LiteLLM also provides `image_generation()` for image models and `speech()` for TTS, so your entire AI toolkit stays unified."
  },
  {
   "cell_type": "markdown",
   "id": "dalle-header",
   "metadata": {},
   "source": "---\n\n## 3. Image Generation with LiteLLM\n\nLiteLLM's `image_generation()` function wraps image models like `gpt-image-1-mini` with the same unified interface. We send a prompt, and get back an image.\n\n**Cost:** ~$0.005 per image (1024x1024, low quality)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dalle-demo",
   "metadata": {},
   "outputs": [],
   "source": "# Generate an image\nresponse = image_generation(\n    model=\"gpt-image-1-mini\",\n    prompt=\"The Taj Mahal at sunset with birds flying, vibrant colors\",\n    quality=\"low\"\n)\n\nimage = Image.open(BytesIO(base64.b64decode(response.data[0].b64_json)))\ndisplay(image)"
  },
  {
   "cell_type": "markdown",
   "id": "tts-header",
   "metadata": {},
   "source": "---\n\n## 4. Text-to-Speech with LiteLLM\n\nLiteLLM's `speech()` function wraps TTS models like `gpt-4o-mini-tts-2025-03-20`. You choose a voice and send the text.\n\n**Available voices:** alloy, ash, ballad, coral, echo, fable, onyx, nova, sage, shimmer, verse  \n**Cost:** ~$0.016 per 1,000 characters"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tts-demo",
   "metadata": {},
   "outputs": [],
   "source": "# Generate and play speech\naudio = speech(\n    model=\"openai/gpt-4o-mini-tts-2025-03-20\",\n    voice=\"alloy\",\n    input=\"Welcome to the AI Systems Engineering course at CV Raman University!\"\n)\n\naudio.stream_to_file(\"welcome.mp3\")\nAudio(\"welcome.mp3\")"
  },
  {
   "cell_type": "markdown",
   "id": "cost-header",
   "metadata": {},
   "source": "---\n\n## 5. Cost Awareness\n\nAI APIs charge per use. Here's a quick reference so you can estimate costs before running code:\n\n| Feature | Approximate Cost | Example |\n|---------|------------------|--------|\n| GPT-4o-mini (text) | ~$0.001 per request | Chat response |\n| Gemini Flash | Free tier available | Chat response |\n| gpt-image-1-mini (low) | ~$0.005 per image | One generated image |\n| gpt-4o-mini-tts-2025-03-20 | ~$0.016 per 1K chars | ~1 paragraph of audio |\n\n> **Tip:** During development and learning, use the cheapest models (GPT-4o-mini, Gemini Flash, gpt-image-1-mini with low quality). Save expensive calls (GPT-4o, high-quality images) for when you really need them."
  },
  {
   "cell_type": "markdown",
   "id": "exercise-header",
   "metadata": {},
   "source": "---\n\n## 6. Exercises\n\n### Exercise 1: Compare Models on the Same Prompt\n\nSend the same prompt to both OpenAI and Gemini using LiteLLM, and compare their responses.\n\n**Steps:**\n1. Pick a prompt (e.g., \"Explain gravity to a 5-year-old in 2 sentences\")\n2. Call `completion()` with `openai/gpt-4o-mini` and `gemini/gemini-2.0-flash`\n3. Print both responses and compare them — which do you prefer?"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exercise",
   "metadata": {},
   "outputs": [],
   "source": "# Exercise 1: Compare responses from two models\n# Try your own prompt — replace the example below\n\n# prompt = \"Explain gravity to a 5-year-old in 2 sentences.\"\n# models = [\"openai/gpt-4o-mini\", \"gemini/gemini-2.0-flash\"]\n\n# for model in models:\n#     response = completion(model=model, messages=[{\"role\": \"user\", \"content\": prompt}])\n#     print(f\"--- {model} ---\")\n#     print(response.choices[0].message.content)\n#     print()"
  },
  {
   "cell_type": "markdown",
   "id": "bsa57b0apuo",
   "source": "### Exercise 2: Generate an Image\n\nUse `image_generation()` from Section 3 to create an image of your choice.\n\n**Steps:**\n1. Write a descriptive prompt (be specific -- colors, style, scene)\n2. Call `image_generation()` with your prompt\n3. Decode and display the result",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "k9r6gc68qy",
   "source": "# Exercise 2: Generate an image with your own prompt\n# Try describing a scene, place, or object in detail\n\n# response = image_generation(\n#     model=\"gpt-image-1-mini\",\n#     prompt=\"your prompt here\",\n#     quality=\"low\"\n# )\n# image = Image.open(BytesIO(base64.b64decode(response.data[0].b64_json)))\n# display(image)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "takeaways",
   "metadata": {},
   "source": "---\n\n## Key Takeaways\n\n1. **LiteLLM provides a unified interface** -- `completion()` for chat, `image_generation()` for images, and `speech()` for TTS — one library for 100+ models across providers\n\n2. **Generate images from text** -- call `image_generation()` with a model and descriptive prompt\n\n3. **Create natural audio** -- call `speech()` to convert any text to speech\n\n4. **Always be cost-aware** -- know the price of each API call before running it\n\n### What's Next?\n\nYou've completed Unit 2! In Unit 3, we'll explore:\n- Open-source models on Hugging Face\n- Model fine-tuning basics\n- Evaluation and benchmarking\n\n---\n\n## Additional Resources\n\n- [LiteLLM Documentation](https://docs.litellm.ai/)\n- [Image Generation Guide](https://platform.openai.com/docs/guides/image-generation)\n- [Text-to-Speech Guide](https://platform.openai.com/docs/guides/text-to-speech)\n\n---\n\n**Course Information:**\n- **Institution:** CV Raman Global University, Bhubaneswar\n- **Program:** AI Center of Excellence\n- **Course:** AI Systems Engineering 1\n- **Developed by:** [Poorit Technologies](https://poorit.in) - *Transform Graduates into Industry-Ready Professionals*\n\n---"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbformat_minor": 5,
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}