{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": "<div align=\"center\">\n<img src=\"https://poorit.in/image.png\" alt=\"Poorit\" width=\"40\" style=\"vertical-align: middle;\"> <b>AI SYSTEMS ENGINEERING 1</b>\n\n## Unit 2: Beyond Text - Images, Audio, and Multi-Model Access\n\n**CV Raman Global University, Bhubaneswar**  \n*AI Center of Excellence*\n\n---\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Poorit-Technologies/cvraman-coe/blob/main/courses-contents/ai-systems-engineering-1/unit-2/03-ai-systems-engineering-1-unit2-beyond-text.ipynb)\n\n</div>\n\n---\n\n### What You'll Learn\n\nIn this notebook, you will:\n\n1. **Use LiteLLM** to access multiple LLM providers with one interface\n2. **Generate images** from text prompts using gpt-image-1-mini\n3. **Create audio with text-to-speech** using OpenAI's TTS\n4. **Practice** with hands-on exercises\n\n**Duration:** ~45 minutes\n\n---"
  },
  {
   "cell_type": "markdown",
   "id": "setup-header",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "install",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q openai litellm pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import base64\n",
    "from io import BytesIO\n",
    "from getpass import getpass\n",
    "from openai import OpenAI\n",
    "from litellm import completion\n",
    "from PIL import Image\n",
    "from IPython.display import Audio, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "api-key",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure API key\n",
    "api_key = getpass(\"Enter your OpenAI API Key: \")\n",
    "os.environ['OPENAI_API_KEY'] = api_key\n",
    "client = OpenAI(api_key=api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "litellm-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. One Interface for Many Models -- LiteLLM\n",
    "\n",
    "So far we've used the OpenAI Python library directly. But what if you want to switch to Google Gemini, Anthropic Claude, or a local model?\n",
    "\n",
    "**LiteLLM** gives you a single `completion()` function that works with 100+ providers. You just change the model name -- the rest of your code stays the same.\n",
    "\n",
    "| Provider | Model Name in LiteLLM | API Key Env Variable |\n",
    "|----------|----------------------|---------------------|\n",
    "| OpenAI | `openai/gpt-4o-mini` | `OPENAI_API_KEY` |\n",
    "| Google | `gemini/gemini-2.0-flash` | `GOOGLE_API_KEY` |\n",
    "| Anthropic | `anthropic/claude-sonnet-4-5-20250929` | `ANTHROPIC_API_KEY` |\n",
    "| Local (Ollama) | `ollama/llama3.2` | -- |\n",
    "\n",
    "Let's try it with OpenAI:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "litellm-demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LiteLLM uses the same messages format you already know\n",
    "response = completion(\n",
    "    model=\"openai/gpt-4o-mini\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Tell me a fun fact about India in one sentence.\"}]\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)\n",
    "print(f\"\\nTokens used: {response.usage.total_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "litellm-note",
   "metadata": {},
   "source": [
    "> **Key idea:** To switch models, you only change the `model` string.  \n",
    "> For example, replacing `\"openai/gpt-4o-mini\"` with `\"gemini/gemini-2.0-flash\"` (after setting `GOOGLE_API_KEY`) would route the same request to Google's API -- no other code changes needed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dalle-header",
   "metadata": {},
   "source": "---\n\n## 3. Image Generation with GPT Image\n\n`gpt-image-1-mini` generates images from text descriptions. We send a prompt, and get back an image.\n\n**Cost:** ~$0.005 per image (1024x1024, low quality)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dalle-function",
   "metadata": {},
   "outputs": [],
   "source": "def generate_image(prompt):\n    \"\"\"Generate an image using gpt-image-1-mini.\"\"\"\n    response = client.images.generate(\n        model=\"gpt-image-1-mini\",\n        prompt=prompt,\n        size=\"1024x1024\",\n        quality=\"low\",\n        n=1\n    )\n\n    image_data = base64.b64decode(response.data[0].b64_json)\n    return Image.open(BytesIO(image_data))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dalle-demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate an image\n",
    "image = generate_image(\"The Taj Mahal at sunset with birds flying, vibrant colors\")\n",
    "display(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tts-header",
   "metadata": {},
   "source": "---\n\n## 4. Text-to-Speech\n\nOpenAI's `gpt-4o-mini-tts` converts text into natural-sounding speech. You choose a voice and send the text.\n\n**Available voices:** alloy, ash, ballad, coral, echo, fable, onyx, nova, sage, shimmer, verse  \n**Cost:** ~$0.016 per 1,000 characters"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tts-function",
   "metadata": {},
   "outputs": [],
   "source": "def text_to_speech(text, voice=\"alloy\"):\n    \"\"\"Convert text to speech audio.\"\"\"\n    response = client.audio.speech.create(\n        model=\"gpt-4o-mini-tts-2025-03-20\",\n        voice=voice,\n        input=text\n    )\n    return response.content"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tts-demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate and play speech\n",
    "audio_content = text_to_speech(\n",
    "    \"Welcome to the AI Systems Engineering course at CV Raman University!\"\n",
    ")\n",
    "\n",
    "# Save and play\n",
    "with open(\"welcome.mp3\", \"wb\") as f:\n",
    "    f.write(audio_content)\n",
    "\n",
    "Audio(\"welcome.mp3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cost-header",
   "metadata": {},
   "source": "---\n\n## 5. Cost Awareness\n\nAI APIs charge per use. Here's a quick reference so you can estimate costs before running code:\n\n| Feature | Approximate Cost | Example |\n|---------|------------------|--------|\n| GPT-4o-mini (text) | ~$0.001 per request | Chat response |\n| Gemini Flash | Free tier available | Chat response |\n| gpt-image-1-mini (low) | ~$0.005 per image | One generated image |\n| gpt-4o-mini-tts | ~$0.016 per 1K chars | ~1 paragraph of audio |\n\n> **Tip:** During development and learning, use the cheapest models (GPT-4o-mini, Gemini Flash, gpt-image-1-mini with low quality). Save expensive calls (GPT-4o, high-quality images) for when you really need them."
  },
  {
   "cell_type": "markdown",
   "id": "exercise-header",
   "metadata": {},
   "source": "---\n\n## 6. Exercises\n\n### Exercise 1: Use Gemini via LiteLLM\n\nUse LiteLLM to call Google's Gemini model. You'll need a Google API key (free tier available).\n\n**Steps:**\n1. Set your `GOOGLE_API_KEY`\n2. Use `completion()` with model `\"gemini/gemini-2.0-flash\"`\n3. Ask it to explain any topic in 2 sentences"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exercise",
   "metadata": {},
   "outputs": [],
   "source": "# Exercise 1: Call Gemini using LiteLLM\n# Hint: It works exactly like the OpenAI call above -- just change the model name\n\n# os.environ['GOOGLE_API_KEY'] = getpass(\"Enter your Google API Key: \")\n\n# response = completion(\n#     model=\"gemini/gemini-2.0-flash\",\n#     messages=[{\"role\": \"user\", \"content\": \"Explain machine learning in 2 sentences.\"}]\n# )\n# print(response.choices[0].message.content)"
  },
  {
   "cell_type": "markdown",
   "id": "bsa57b0apuo",
   "source": "### Exercise 2: Generate an Image\n\nUse the `generate_image()` function from Section 3 to create an image of your choice.\n\n**Steps:**\n1. Write a descriptive prompt (be specific -- colors, style, scene)\n2. Call `generate_image()` with your prompt\n3. Display the result",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "k9r6gc68qy",
   "source": "# Exercise 2: Generate an image with your own prompt\n# Try describing a scene, place, or object in detail\n\n# image = generate_image(\"your prompt here\")\n# display(image)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "takeaways",
   "metadata": {},
   "source": "---\n\n## Key Takeaways\n\n1. **LiteLLM provides a unified interface** -- one `completion()` function for 100+ models; switch providers by changing the model string\n\n2. **gpt-image-1-mini generates images from text** -- use `client.images.generate()` with a descriptive prompt\n\n3. **gpt-4o-mini-tts creates natural audio** -- use `client.audio.speech.create()` to convert any text to speech\n\n4. **Always be cost-aware** -- know the price of each API call before running it\n\n### What's Next?\n\nYou've completed Unit 2! In Unit 3, we'll explore:\n- Open-source models on Hugging Face\n- Model fine-tuning basics\n- Evaluation and benchmarking\n\n---\n\n## Additional Resources\n\n- [LiteLLM Documentation](https://docs.litellm.ai/)\n- [Image Generation Guide](https://platform.openai.com/docs/guides/image-generation)\n- [Text-to-Speech Guide](https://platform.openai.com/docs/guides/text-to-speech)\n\n---\n\n**Course Information:**\n- **Institution:** CV Raman Global University, Bhubaneswar\n- **Program:** AI Center of Excellence\n- **Course:** AI Systems Engineering 1\n- **Developed by:** [Poorit Technologies](https://poorit.in) - *Transform Graduates into Industry-Ready Professionals*\n\n---"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbformat_minor": 5,
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}